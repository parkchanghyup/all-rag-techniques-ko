{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Feedback Loop in RAG\n",
    "\n",
    "이 노트북에서는 시간이 지날수록 스스로 발전하는 피드백 루프 기반의 RAG 시스템을 구현해보겠습니다. 사용자의 피드백을 받아들이고 이를 바탕으로 개선해나가면서, 시스템은 매번 더 정확하고 유용한 답변을 제시하는 법을 배워갑니다.\n",
    "\n",
    "기존의 RAG 시스템은 주로 임베딩 유사도에 따라 정보를 검색하는 방식으로, 변화에 둔감한 면이 있었습니다.   \n",
    "하지만 피드백 루프를 도입하면 시스템은 다음과 같은 방식으로 개선됩니다.\n",
    "1. 잘 작동했던 방식과 그렇지 않았던 방식을 기억하고\n",
    "2. 시간이 지날수록 문서의 중요도를 조정하며\n",
    "3. 효과적인 질문과 답변 쌍을 지식으로 쌓고\n",
    "4. 사용자와의 상호작용을 통해 점점 더 똑똑해집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정하기\n",
    "필요한 라이브러리를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API 클라이언트 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client_openai = OpenAI(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출하기\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 저는 gemini를 이용해 pdf에서 텍스트를 추출하는 방식을 사용합니다.  \n",
    "만약 txt 형태로 파일이 존재한다면 `load_text_file` 함수를 사용하면됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # API 키 설정\n",
    "    genai.configure(api_key=gemini_API_KEY)\n",
    "    client = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "\n",
    "    # PDF 파일 업로드\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        file_data = file.read()\n",
    "\n",
    "\n",
    "    prompt = \"Extract all text from the provided PDF file.\"\n",
    "    response = client.generate_content([\n",
    "        {\"mime_type\": \"application/pdf\", \"data\": file_data},\n",
    "        prompt\n",
    "    ],generation_config={\n",
    "            \"max_output_tokens\": 8192  # 최대 출력 토큰 수 설정 (예: 8192 토큰, 약 24,000~32,000자)\n",
    "    })\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 text 파일로 저장되어 있다면 load_text_file 함수를 사용하면 됩니다.\n",
    "def load_text_file(pdf_path):\n",
    "\n",
    "    # text 파일 로드\n",
    "    with open(pdf_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "        text = txt_file.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "txt_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "extracted_text = load_text_file(txt_path)\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청크 분할\n",
    "텍스트를 추출한 뒤에는 검색 정확도를 높이기 위해 조금씩 겹치도록 나눠서 작은 단위로 분할(chunk)합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n자 단위로, 일부가 겹치도록 chunking 합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청크할 텍스트입니다.\n",
    "    n (int): 각 청크의 문자 수입니다.\n",
    "    overlap (int): 청크 간 겹치는 문자 수입니다.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 청크된 텍스트 리스트입니다.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크된 텍스트를 저장할 리스트\n",
    "    \n",
    "    # overlap만큼 겹치도록 text를 n의 길이로 chunking\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store 구축\n",
    "NumPy를 사용하여 간단한 Vecotr store 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용하여 간단한 Vecotr store 구축\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소 초기화\n",
    "        \"\"\"\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목 추가\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        쿼리 임베딩과 가장 비슷한 항목들을 코사인 유사도를 이용해 찾는 함수\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): 쿼리 임베딩 벡터.\n",
    "            k (int): 반환할 결과의 수.\n",
    "            filter_func (callable, optional): 메타데이터를 기반으로 결과를 필터링하는 함수.\n",
    "                                            메타데이터 딕셔너리를 입력으로 받아 boolean을 반환.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: 상위 k개 유사 항목, 각 항목은 다음을 포함:\n",
    "                - text: 원본 텍스트\n",
    "                - metadata: 연관된 메타데이터\n",
    "                - similarity: 코사인 유사도 점수\n",
    "                - relevance_score: 메타데이터 기반 관련성 또는 계산된 유사도\n",
    "                \n",
    "        Note: 벡터가 저장되어 있지 않거나 필터를 통과하지 못하면 빈 리스트를 반환.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 벡터 저장소가 비어있으면 빈 리스트 반환\n",
    "        \n",
    "        # 쿼리 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 쿼리와 각 저장된 벡터 사이의 코사인 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # 필터 기준을 통과하지 못하는 항목 건너뛰기\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # 코사인 유사도 계산: dot product / (norm1 * norm2)\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # 인덱스와 유사도 점수 저장\n",
    "        \n",
    "        # 유사도 점수에 따라 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 딕셔너리 생성\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score,\n",
    "                # 메타데이터에서 이미 존재하는 관련성 점수가 있으면 사용, 없으면 유사도 점수 사용\n",
    "                \"relevance_score\": self.metadata[idx].get(\"relevance_score\", score)\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def create_embeddings(embedding_model, texts, device='cuda', batch_size=16):\n",
    "    \"\"\"\n",
    "    SentenceTransformer 모델을 사용하여 지정된 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        embedding_model: 임베딩을 생성할 SentenceTransformer 모델입니다.\n",
    "        texts (list): 임베딩을 생성할 입력 텍스트 리스트입니다.\n",
    "        device (str): 모델을 실행할 장치 ('cuda' for GPU, 'cpu' for CPU).\n",
    "        batch_size (int): 인코딩을 위한 배치 크기입니다.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 모델에 의해 생성된 임베딩입니다.\n",
    "    \"\"\"\n",
    "    # 모델이 지정된 장치에 있는지 확인합니다.\n",
    "    embedding_model = embedding_model.to(device)\n",
    "    \n",
    "    # 지정된 배치 크기로 임베딩을 생성합니다.\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        device=device,\n",
    "        batch_size=batch_size,  # 메모리 사용량을 줄이기 위해 더 작은 배치 크기를 사용합니다.\n",
    "        show_progress_bar=True  # 인코딩 진행 상태를 모니터링하기 위한 진행 표시줄을 표시합니다.\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# GPU 사용 가능 여부를 확인합니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델을 로드합니다.\n",
    "model = \"BAAI/bge-m3\"\n",
    "embedding_model = SentenceTransformer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback System Functions\n",
    "이제 피드백 시스템의 핵심 요소들을 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feedback(query, response, relevance, quality, comments=\"\"):\n",
    "    \"\"\"\n",
    "    사용자 피드백을 보기 쉽게 딕셔너리 형태로 정리합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        response (str): 시스템 응답\n",
    "        relevance (int): 관련성 점수 (1-5)\n",
    "        quality (int): 품질 점수 (1-5)\n",
    "        comments (str): 선택적 피드백 코멘트\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 사용자 피드백\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"relevance\": int(relevance),\n",
    "        \"quality\": int(quality),\n",
    "        \"comments\": comments,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def store_feedback(feedback, feedback_file=\"feedback_data.json\"):\n",
    "    \"\"\"\n",
    "    피드백을 JSON 파일에 저장\n",
    "    \n",
    "    Args:\n",
    "        feedback (Dict): 피드백 데이터\n",
    "        feedback_file (str): 피드백 파일 경로\n",
    "    \"\"\"\n",
    "    with open(feedback_file, \"a\") as f:\n",
    "        json.dump(feedback, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_feedback_data(feedback_file=\"feedback_data.json\"):\n",
    "    \"\"\"\n",
    "    피드백 데이터를 파일에서 로드\n",
    "    \n",
    "    Args:\n",
    "        feedback_file (str): feedback file 경로\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 피드백 리스트\n",
    "    \"\"\"\n",
    "    feedback_data = []\n",
    "    try:\n",
    "        with open(feedback_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    feedback_data.append(json.loads(line.strip()))\n",
    "    except FileNotFoundError:\n",
    "        print(\"No feedback data file found. Starting with empty feedback.\")\n",
    "    \n",
    "    return feedback_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing with Feedback Awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    RAG를 위한 문서를 처리하고, 피드백 루프까지 포함하는 전체 파이프라인\n",
    "    이 함수는 다음과 같은 과정을 순차적으로 수행합니다:\n",
    "    1. PDF 파일에서 텍스트 추출\n",
    "    2. 텍스트를 일정 단위로 나누되, 중간에 겹치는 부분을 포함\n",
    "    3. 각 조각에 대한 임베딩 생성\n",
    "    4. 메타데이터와 함께 벡터 데이터베이스에 저장\n",
    "\n",
    "    Args:\n",
    "    file_path (str): 파일 경로\n",
    "    chunk_size (int): 각 텍스트 청크의 크기(문자)\n",
    "    chunk_overlap (int): 연속된 청크 간 중복 범위(문자)\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], SimpleVectorStore]: 튜플에 포함된 요소:\n",
    "        - 문서 청크 리스트\n",
    "        - 임베딩과 메타데이터가 포함된 벡터 저장소\n",
    "    \"\"\"\n",
    "    # 1. PDF에서 텍스트 추출\n",
    "    #print(\"Extracting text from PDF...\")\n",
    "    #extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # 1. 텍스트 파일 로드\n",
    "    extracted_text = load_text_file(file_path)\n",
    "    \n",
    "    # 2. 청크로 분할\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # 3. 각 청크에 대한 임베딩 생성\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(embedding_model, chunks, device=device, batch_size=1)\n",
    "    \n",
    "    # 4. 벡터 저장소 초기화\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # 5. 각 청크와 임베딩을 벡터 저장소에 추가\n",
    "    # 피드백 기반 개선을 위한 메타데이터 포함\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\n",
    "                \"index\": i,                # 원본 문서에서의 위치\n",
    "                \"source\": file_path,        # 원본 문서 경로\n",
    "                \"relevance_score\": 1.0,    # 초기 relevance score (피드백에 따라 업데이트됨)\n",
    "                \"feedback_count\": 0        # 이 청크에 대해 받은 피드백 수\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return chunks, store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Adjustment Based on Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_feedback_relevance(query, doc_text, feedback):\n",
    "    \"\"\"\n",
    "    LLM을 활용해 이전 피드백이 현재 쿼리와 문서에 얼마나 관련 있는지를 판단합니다.\n",
    "    \n",
    "    이 함수는 현재 쿼리와 문서 내용, 그리고 과거의 쿼리 및 피드백을 함께 LLM에 전달하여,\n",
    "    어떤 피드백이 이번 검색 결과에 영향을 줄 수 있을지를 평가합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 현재 사용자 쿼리\n",
    "        doc_text (str): 평가할 문서의 텍스트 내용\n",
    "        feedback (Dict): 'query'와 'response' 키가 포함된 이전 피드백 데이터\n",
    "        \n",
    "    Returns:\n",
    "        bool: 피드백이 현재 쿼리와 문서와 관련 있는지 여부\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"You are an AI system that determines if a past feedback is relevant to a current query and document.\n",
    "    Answer with ONLY 'yes' or 'no'. Your job is strictly to determine relevance, not to provide explanations.\"\"\"\n",
    "\n",
    "    # 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"\n",
    "    Current query: {query}\n",
    "    Past query that received feedback: {feedback['query']}\n",
    "    Document content: {doc_text[:500]}... [truncated]\n",
    "    Past response that received feedback: {feedback['response'][:500]}... [truncated]\n",
    "\n",
    "    Is this past feedback relevant to the current query and document? (yes/no)\n",
    "    \"\"\"\n",
    "\n",
    "    # 평가\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0  \n",
    "    )\n",
    "    \n",
    "    \n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    return 'yes' in answer  # 'yes'가 포함되어 있으면 True 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_relevance_scores(query, results, feedback_data):\n",
    "    \"\"\"\n",
    "    과거 피드백을 반영해 문서의 연관성 점수를 조정하고, 검색 결과의 품질을 높입니다.\n",
    "\n",
    "    이 함수는 이전 사용자 피드백을 분석해, 현재 쿼리와 관련된 내용을 선별합니다.\n",
    "    그 후 피드백의 평가 기준에 따라 점수 보정값을 계산하고, 이를 기반으로 검색 결과의 순위를 다시 매깁니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 현재 사용자 쿼리\n",
    "        results (List[Dict]): 유사도 점수가 포함된 검색된 문서 리스트\n",
    "        feedback_data (List[Dict]): 사용자 평가가 포함된 과거 피드백 리스트\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 관련성 점수가 조정된 결과, 새로운 점수에 따라 정렬됨\n",
    "    \"\"\"\n",
    "    # 피드백 데이터가 없으면 원본 결과 반환\n",
    "    if not feedback_data:\n",
    "        return results\n",
    "    \n",
    "    print(\"Adjusting relevance scores based on feedback history...\")\n",
    "    \n",
    "    # 각 검색된 문서에 대해 처리\n",
    "    for i, result in enumerate(results):\n",
    "        document_text = result[\"text\"]\n",
    "        relevant_feedback = []\n",
    "        \n",
    "        # 해당 문서와 쿼리 조합에 대해,\n",
    "        # 각 과거 피드백이 얼마나 관련 있는지를 LLM을 통해 평가하고 관련 피드백을 찾아냅니다.\n",
    "        for feedback in feedback_data:\n",
    "            is_relevant = assess_feedback_relevance(query, document_text, feedback)\n",
    "            if is_relevant:\n",
    "                relevant_feedback.append(feedback)\n",
    "        \n",
    "        # 관련성 있는 피드백이 있으면 점수 조정\n",
    "        if relevant_feedback:\n",
    "            # 모든 적용 가능한 피드백 항목에서 평균 관련성 등급 계산\n",
    "            # 피드백 관련성은 1~5 범위 (1:관련 없음, 5:매우 관련 있음)\n",
    "            avg_relevance = sum(f['relevance'] for f in relevant_feedback) / len(relevant_feedback)\n",
    "            \n",
    "            # 평균 관련성을 0.5-1.5 범위의 점수 수정자로 변환\n",
    "            # - 3/5 미만의 점수는 원래 유사도를 감소시킴 (modifier < 1.0)\n",
    "            # - 3/5 이상의 점수는 원래 유사도를 증가시킴 (modifier > 1.0)\n",
    "            modifier = 0.5 + (avg_relevance / 5.0)\n",
    "            \n",
    "            # 원래 유사도에 modifier 적용\n",
    "            original_score = result[\"similarity\"]\n",
    "            adjusted_score = original_score * modifier\n",
    "            \n",
    "            # 결과 딕셔너리에 새로운 점수와 피드백 메타데이터 업데이트\n",
    "            result[\"original_similarity\"] = original_score  # 원래 유사도 유지\n",
    "            result[\"similarity\"] = adjusted_score           # 기본 점수 업데이트\n",
    "            result[\"relevance_score\"] = adjusted_score      # 관련성 점수 업데이트\n",
    "            result[\"feedback_applied\"] = True               # 피드백 적용 표시\n",
    "            result[\"feedback_count\"] = len(relevant_feedback)  # 사용된 피드백 항목 수\n",
    "            \n",
    "            # 유사도 조정에 대한 세부 정보 logging\n",
    "            print(f\"  Document {i+1}: Adjusted score from {original_score:.4f} to {adjusted_score:.4f} based on {len(relevant_feedback)} feedback(s)\")\n",
    "    \n",
    "    # 조정된 유사도 점수에 따라 결과 다시 정렬\n",
    "    results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Our Index with Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_index(current_store, chunks, feedback_data):\n",
    "    \"\"\"\n",
    "    시간이 지남에 따라 검색 품질을 높일 수 있도록, 우수한 피드백을 벡터 스토어에 반영합니다.\n",
    "    \n",
    "    이 함수는 다음과 같은 방식으로 지속적인 학습을 수행합니다:\n",
    "    1. 높은 평가를 받은 Q&A 쌍 등, 품질이 검증된 피드백을 선별\n",
    "    2. 성공적인 상호작용에서 새로운 검색 항목을 생성\n",
    "    3. 이 항목들을 강화된 가중치와 함께 벡터 스토어에 추가\n",
    "    \n",
    "    Args:\n",
    "        current_store (SimpleVectorStore): 원본 문서 청크가 포함된 현재 벡터 저장소\n",
    "        chunks (List[str]): 원본 문서 텍스트 청크\n",
    "        feedback_data (List[Dict]): 관련성과 품질 등급이 포함된 과거 사용자 피드백\n",
    "        \n",
    "    Returns:\n",
    "        SimpleVectorStore: 원본 청크와 피드백 기반 콘텐츠가 포함된 개선된 벡터 저장소\n",
    "    \"\"\"\n",
    "    print(\"Fine-tuning index with high-quality feedback...\")\n",
    "    \n",
    "    # 연관도와 응답 품질이 모두 4점 이상인, 우수한 응답만 선별합니다.\n",
    "    # 가장 성공적인 상호작용만을 학습에 반영하기 위한 과정입니다.\n",
    "    good_feedback = [f for f in feedback_data if f['relevance'] >= 4 and f['quality'] >= 4]\n",
    "    \n",
    "    if not good_feedback:\n",
    "        print(\"No high-quality feedback found for fine-tuning.\")\n",
    "        return current_store  # 좋은 피드백이 없으면 원본 저장소 반환\n",
    "    \n",
    "    # 원본 청크와 피드백 기반 콘텐츠가 포함된 새로운 저장소 초기화\n",
    "    new_store = SimpleVectorStore()\n",
    "    \n",
    "    # 먼저 원본 문서 청크와 기존 메타데이터를 전송\n",
    "    for i in range(len(current_store.texts)):\n",
    "        new_store.add_item(\n",
    "            text=current_store.texts[i],\n",
    "            embedding=current_store.vectors[i],\n",
    "            metadata=current_store.metadata[i].copy()  # 참조 문제를 피하기 위해 복사본을 사용합니다.\n",
    "        )\n",
    "    \n",
    "    # 좋은 피드백에서 향상된 콘텐츠 생성 및 추가\n",
    "    for feedback in good_feedback:\n",
    "        # 질문과 고품질 답변을 결합하는 새로운 문서 포맷팅\n",
    "        # 이는 사용자 쿼리에 직접 주소를 지정하는 검색 가능한 콘텐츠를 생성합니다\n",
    "        enhanced_text = f\"Question: {feedback['query']}\\nAnswer: {feedback['response']}\"\n",
    "        \n",
    "        # 이 새로운 문서에 대한 임베딩 벡터 생성\n",
    "        embedding = create_embeddings(embedding_model, chunks, device=device, batch_size=1)[0]\n",
    "        \n",
    "        # 원본 출처와 중요성을 식별하는 특수 메타데이터로 벡터 저장소에 추가\n",
    "        new_store.add_item(\n",
    "            text=enhanced_text,\n",
    "            embedding=embedding,\n",
    "            metadata={\n",
    "                \"type\": \"feedback_enhanced\",  # 피드백에서 파생됨\n",
    "                \"query\": feedback[\"query\"],   # 원본 쿼리 저장\n",
    "                \"relevance_score\": 1.2,       # 초기 관련성 증가\n",
    "                \"feedback_count\": 1,          # 피드백 통합 추적\n",
    "                \"original_feedback\": feedback # 완전한 피드백 레코드 유지\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"Added enhanced content from feedback: {feedback['query'][:50]}...\")\n",
    "    \n",
    "    # 개선에 대한 요약 통계 logging\n",
    "    print(f\"Fine-tuned index now has {len(new_store.texts)} items (original: {len(chunks)})\")\n",
    "    return new_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with Feedback Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model_name='gpt-4.1-nano'):\n",
    "\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"당신은 제공된 Context에 기반하여 답변하는 AI 어시스턴트입니다. 답변이 컨텍스트에서 직접 도출될 수 없는 경우, 다음 문장을 사용하세요: '해당 질문에 답변할 충분한 정보가 없습니다.'\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please answer the question based only on the context provided above. Be concise and accurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_feedback_loop(query, vector_store, feedback_data, k=5, model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    피드백 루프를 포함한 RAG 파이프라인을 구현합니다.    \n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        vector_store (SimpleVectorStore): 문서 청크가 포함된 벡터 저장소\n",
    "        feedback_data (List[Dict]): 피드백 기록\n",
    "        k (int): 검색할 문서 수\n",
    "        model (str): 응답 생성을 위한 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 쿼리, 검색된 문서, 응답이 포함된 결과\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing query with feedback-enhanced RAG ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # 1. 쿼리 임베딩 생성\n",
    "    query_embedding = create_embeddings(embedding_model, [query], device=device, batch_size=1)[0]\n",
    "    \n",
    "    # 2. 쿼리 임베딩을 기반으로 초기 검색 수행\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    # 3. 피드백을 기반으로 검색된 문서의 관련성 점수 조정\n",
    "    adjusted_results = adjust_relevance_scores(query, results, feedback_data)\n",
    "    \n",
    "    # 4. 조정된 결과에서 텍스트 추출하여 컨텍스트 생성\n",
    "    retrieved_texts = [result[\"text\"] for result in adjusted_results]\n",
    "    \n",
    "    # 5. 검색된 텍스트를 연결하여 응답 생성을 위한 컨텍스트 생성\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    # 6. 컨텍스트와 쿼리를 사용하여 응답 생성\n",
    "    print(\"Generating response...\")\n",
    "    response = generate_response(query, context, model_name)\n",
    "    \n",
    "    # 7. 최종 결과 컴파일\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": adjusted_results,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Response ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Workflow: From Initial Setup to Feedback Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_rag_workflow(file_path, query, feedback_data=None, feedback_file=\"feedback_data.json\", fine_tune=False):\n",
    "    \"\"\"\n",
    "    피드백을 통합해 지속적으로 개선되는 RAG 워크플로우 전체를 실행합니다.\n",
    "    이 함수는 Retrieval-Augmented Generation 과정을 다음 단계에 따라 종합적으로 수행합니다:\n",
    "\n",
    "    1. 기존 피드백 데이터 불러오기\n",
    "    2. 문서를 처리하고 chunk 단위로 분할\n",
    "    3. 이전 피드백을 반영해 벡터 인덱스를 선택적으로 보정\n",
    "    4. 피드백 기반 연관성 점수를 활용해 검색 및 생성 수행\n",
    "    5. 새로운 사용자 피드백 수집\n",
    "    6. 시스템이 점차 학습할 수 있도록 피드백 저장\n",
    "\n",
    "    Args:\n",
    "        file_path (str): 파일 경로\n",
    "        query (str): 사용자 쿼리\n",
    "        feedback_data (List[Dict], optional): 이미 로드된 피드백 데이터, None인 경우 파일에서 로드\n",
    "        feedback_file (str): 피드백 기록을 저장하는 JSON 파일 경로\n",
    "        fine_tune (bool): 성공적인 과거 Q&A 쌍을 활용하여 인덱스를 개선할지 여부\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 응답과 검색 메타데이터가 포함된 결과\n",
    "    \"\"\"\n",
    "    # 1. 기존 피드백 데이터 불러오기\n",
    "    if feedback_data is None:\n",
    "        feedback_data = load_feedback_data(feedback_file)\n",
    "        print(f\"Loaded {len(feedback_data)} feedback entries from {feedback_file}\")\n",
    "    \n",
    "    # 2. 문서를 처리하고 chunk 단위로 분할\n",
    "    chunks, vector_store = process_document(file_path)\n",
    "    \n",
    "    # 3. 이전 피드백을 반영해 벡터 인덱스를 선택적으로 보정\n",
    "    if fine_tune and feedback_data:\n",
    "        vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
    "    \n",
    "    # 4. 피드백 기반 연관성 점수를 활용해 검색 및 생성 수행\n",
    "    result = rag_with_feedback_loop(query, vector_store, feedback_data)\n",
    "    \n",
    "    # 5. 새로운 사용자 피드백 수집\n",
    "    print(\"\\n=== Would you like to provide feedback on this response? ===\")\n",
    "    print(\"Rate relevance (1-5, with 5 being most relevant):\")\n",
    "    relevance = input()\n",
    "    \n",
    "    print(\"Rate quality (1-5, with 5 being highest quality):\")\n",
    "    quality = input()\n",
    "    \n",
    "    print(\"Any comments? (optional, press Enter to skip)\")\n",
    "    comments = input()\n",
    "    \n",
    "    # 6. 피드백을 구조화된 데이터로 포맷팅\n",
    "    feedback = get_user_feedback(\n",
    "        query=query,\n",
    "        response=result[\"response\"],\n",
    "        relevance=int(relevance),\n",
    "        quality=int(quality),\n",
    "        comments=comments\n",
    "    )\n",
    "    \n",
    "    # 7. 시스템이 점차 학습할 수 있도록 피드백 저장\n",
    "    store_feedback(feedback, feedback_file)\n",
    "    print(\"Feedback recorded. Thank you!\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Feedback Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feedback_loop(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    피드백 루프가 RAG 성능에 미치는 영향을 평가합니다.\n",
    "    \n",
    "    이 함수는 피드백 적용 전후의 응답 결과를 비교하여, 피드백이 검색 및 생성 품질에 어떤 영향을 주는지 확인하는 실험을 수행합니다.\n",
    "    \n",
    "    실행 흐름:\n",
    "    1. 피드백 없이 테스트 쿼리 실행\n",
    "    2. 기준 답변이 있을 경우, 이를 기반으로 피드백 생성\n",
    "    3. 생성된 피드백을 활용하여 동일한 쿼리를 다시 실행\n",
    "    4. 두 결과를 비교하여 피드백 효과를 정량적으로 분석\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): 검색 대상이 되는 PDF 문서 경로\n",
    "        test_queries (List[str]): 테스트할 질의 리스트\n",
    "        reference_answers (List[str], optional): 기준 정답. 제공되면 피드백 생성 및 성능 평가에 활용\n",
    "\n",
    "    Returns:\n",
    "        Dict: 평가 결과 딕셔너리. 아래 내용을 포함합니다.\n",
    "            - round1_results: 피드백 없이 실행한 결과\n",
    "            - round2_results: 피드백을 적용한 결과\n",
    "            - comparison: 두 결과 간의 비교 분석\n",
    "    \"\"\"\n",
    "    print(\"=== 피드백 루프 영향 평가 시작 ===\")\n",
    "    \n",
    "    # 임시 피드백 파일 생성 (이번 평가에서만 사용)\n",
    "    temp_feedback_file = \"temp_evaluation_feedback.json\"\n",
    "    \n",
    "    # 초기 피드백 데이터 (비어 있음)\n",
    "    feedback_data = []\n",
    "    \n",
    "    # ----------------------- 1차 실행: 피드백 없이 -----------------------\n",
    "    print(\"\\n=== 1차 실행: 피드백 없음 ===\")\n",
    "    round1_results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\nQuery {i+1}: {query}\")\n",
    "        \n",
    "        # 문서 처리 및 벡터스토어 생성\n",
    "        chunks, vector_store = process_document(pdf_path)\n",
    "        \n",
    "        # 피드백 없이 RAG 실행\n",
    "        result = rag_with_feedback_loop(query, vector_store, [])\n",
    "        round1_results.append(result)\n",
    "        \n",
    "        # 기준 답변이 있다면, 이를 바탕으로 피드백 생성\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            similarity_to_ref = calculate_similarity(result[\"response\"], reference_answers[i])\n",
    "            relevance = max(1, min(5, int(similarity_to_ref * 5)))\n",
    "            quality = max(1, min(5, int(similarity_to_ref * 5)))\n",
    "            \n",
    "            feedback = get_user_feedback(\n",
    "                query=query,\n",
    "                response=result[\"response\"],\n",
    "                relevance=relevance,\n",
    "                quality=quality,\n",
    "                comments=f\"기준 답변과 유사도 기반 피드백: {similarity_to_ref:.2f}\"\n",
    "            )\n",
    "            \n",
    "            feedback_data.append(feedback)\n",
    "            store_feedback(feedback, temp_feedback_file)\n",
    "    \n",
    "    # ----------------------- 2차 실행: 피드백 적용 -----------------------\n",
    "    print(\"\\n=== 2차 실행: 피드백 적용 ===\")\n",
    "    round2_results = []\n",
    "    \n",
    "    # 문서를 다시 처리하고, 피드백 기반으로 인덱스를 보완\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\nQuery {i+1}: {query}\")\n",
    "        \n",
    "        result = rag_with_feedback_loop(query, vector_store, feedback_data)\n",
    "        round2_results.append(result)\n",
    "    \n",
    "    # ----------------------- 결과 비교 -----------------------\n",
    "    comparison = compare_results(test_queries, round1_results, round2_results, reference_answers)\n",
    "    \n",
    "    # 임시 피드백 파일 정리\n",
    "    if os.path.exists(temp_feedback_file):\n",
    "        os.remove(temp_feedback_file)\n",
    "    \n",
    "    return {\n",
    "        \"round1_results\": round1_results,\n",
    "        \"round2_results\": round2_results,\n",
    "        \"comparison\": comparison\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    두 텍스트 간 의미적 유사도를 임베딩을 사용하여 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        text1 (str): 첫 번째 텍스트\n",
    "        text2 (str): 두 번째 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        float: 0과 1 사이의 유사도 점수\n",
    "    \"\"\"\n",
    "    # 두 텍스트의 임베딩 생성\n",
    "    embedding1 = create_embeddings(embedding_model, [text1], device=device, batch_size=1)[0]\n",
    "    embedding2 = create_embeddings(embedding_model, [text2], device=device, batch_size=1)[0]\n",
    "    \n",
    "    # 임베딩을 numpy 배열로 변환\n",
    "    vec1 = np.array(embedding1)\n",
    "    vec2 = np.array(embedding2)\n",
    "    \n",
    "    # 두 벡터 간 코사인 유사도 계산\n",
    "    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(queries, round1_results, round2_results, reference_answers=None):\n",
    "    \"\"\"\n",
    "    피드백 적용 전후의 RAG 응답을 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        queries (List[str]): 테스트에 사용된 질의 리스트\n",
    "        round1_results (List[Dict]): 피드백 없이 실행한 응답 결과\n",
    "        round2_results (List[Dict]): 피드백 적용 후의 응답 결과\n",
    "        reference_answers (List[str], optional): 기준 정답 (있을 경우 비교 분석에 활용)\n",
    "        \n",
    "    Returns:\n",
    "        str: 응답 품질 비교에 대한 분석 결과 목록\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 결과 비교 시작 ===\")\n",
    "    \n",
    "    # 분석 역할을 부여할 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 RAG 시스템 응답을 평가하는 전문가입니다. 다음 두 응답을 비교해주세요:\n",
    "        1. 일반 RAG: 피드백 없이 수행된 응답\n",
    "        2. 피드백 강화 RAG: 피드백 루프를 통해 개선된 응답\n",
    "\n",
    "        아래 기준에 따라 어떤 응답이 더 나은지 분석해 주세요:\n",
    "        - 질의와의 관련성\n",
    "        - 정보의 정확성\n",
    "        - 내용의 충실도\n",
    "        - 표현의 명확성과 간결함\n",
    "    \"\"\"\n",
    "\n",
    "    comparisons = []\n",
    "    \n",
    "    # 각 쿼리에 대해 응답을 비교하고 분석 생성\n",
    "    for i, (query, r1, r2) in enumerate(zip(queries, round1_results, round2_results)):\n",
    "        # 비교 대상이 되는 두 응답을 포함한 프롬프트 작성\n",
    "        comparison_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "\n",
    "        일반 RAG 응답:\n",
    "        {r1[\"response\"]}\n",
    "\n",
    "        피드백 강화 RAG 응답:\n",
    "        {r2[\"response\"]}\n",
    "        \"\"\"\n",
    "\n",
    "        # 기준 정답이 있다면 함께 제시\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            comparison_prompt += f\"\"\"\n",
    "            기준 정답:\n",
    "            {reference_answers[i]}\n",
    "            \"\"\"\n",
    "\n",
    "        # 비교 지시 문장 추가\n",
    "        comparison_prompt += \"\"\"\n",
    "        위 두 응답을 비교하고, 어떤 응답이 더 나은지 그 이유와 함께 설명해주세요.\n",
    "        특히 피드백 루프가 응답 품질에 어떤 영향을 미쳤는지에 집중해주세요.\n",
    "        \"\"\"\n",
    "\n",
    "        # OpenAI API를 호출하여 분석 요청\n",
    "        response = client_openai.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": comparison_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # 분석 결과 저장\n",
    "        comparisons.append({\n",
    "            \"query\": query,\n",
    "            \"analysis\": response.choices[0].message.content\n",
    "        })\n",
    "        \n",
    "        # 간략 출력 (200자까지)\n",
    "        print(f\"\\nQuery {i+1}: {query}\")\n",
    "        print(f\"Analysis: {response.choices[0].message.content[:200]}...\")\n",
    "    \n",
    "    return comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the feedback loop (Custom Validation Queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 평가 데이터 로드하기\n",
    "df = pd.read_csv('./data_creation/rag_val_new_post.csv')\n",
    "\n",
    "# test queries 정의\n",
    "test_queries = [\n",
    "    df['query'][0],\n",
    "    # df['query'][1],\n",
    "    # df['query'][2],\n",
    "    # df['query'][3],\n",
    "    # df['query'][4],\n",
    "    # df['query'][5],\n",
    "    # df['query'][6],\n",
    "    # df['query'][7]\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# 평가를 위한 기준 답변 정의\n",
    "reference_answers = [\n",
    "    df['generation_gt'][0],\n",
    "    # df['generation_gt'][1],\n",
    "    # df['generation_gt'][2],\n",
    "    # df['generation_gt'][3],\n",
    "    # df['generation_gt'][4],\n",
    "    # df['generation_gt'][5],\n",
    "    # df['generation_gt'][6],\n",
    "    # df['generation_gt'][7],\n",
    "    # ...\n",
    "    ]\n",
    "\n",
    "\n",
    "# 파일 경로\n",
    "file_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "# 평가 실행\n",
    "evaluation_results = evaluate_feedback_loop(\n",
    "    file_path=file_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# # Run a full RAG workflow\n",
    "########################################\n",
    "\n",
    "# # Run an interactive example\n",
    "# print(\"\\n\\n=== INTERACTIVE EXAMPLE ===\")\n",
    "# print(\"Enter your query about AI:\")\n",
    "# user_query = input()\n",
    "\n",
    "# # Load accumulated feedback\n",
    "# all_feedback = load_feedback_data()\n",
    "\n",
    "# # Run full workflow\n",
    "# result = full_rag_workflow(\n",
    "#     pdf_path=pdf_path,\n",
    "#     query=user_query,\n",
    "#     feedback_data=all_feedback,\n",
    "#     fine_tune=True\n",
    "# )\n",
    "\n",
    "########################################\n",
    "# # Run a full RAG workflow\n",
    "########################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
