{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# RAG 성능 향상을 위한 핵심 문단 추출 (RSE)\n",
    "이 노트북에서는 RAG 시스템의 문맥 품질을 높이기 위한 핵심 문단 추출 (RSE, Relevant Segment Extraction) 기법을 구현합니다.   \n",
    "단순히 관련 청크 몇 개를 가져오는 대신, 문서 내에서 실제로 이어지는 핵심 구간을 찾아 재구성함으로써, LLM이 더 나은 문맥에서 작동할 수 있도록 돕습니다.\n",
    "\n",
    "## 핵심 개념\n",
    "관련 있는 청크들은 보통 문서 안에서도 서로 가까운 위치에 몰려 있는 경우가 많습니다.   \n",
    "이러한 ‘의미 덩어리’를 찾아 그 연속성을 유지하면, LLM에게 더 일관성 있고 이해하기 쉬운 문맥을 제공할 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정하기\n",
    "필요한 라이브러리를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API 클라이언트 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client_openai = OpenAI(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출하기\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 저는 gemini를 이용해 pdf에서 텍스트를 추출하는 방식을 사용합니다.  \n",
    "만약 txt 형태로 파일이 존재한다면 `load_text_file` 함수를 사용하면됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # API 키 설정\n",
    "    genai.configure(api_key=gemini_API_KEY)\n",
    "    client = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "\n",
    "    # PDF 파일 업로드\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        file_data = file.read()\n",
    "\n",
    "\n",
    "    prompt = \"Extract all text from the provided PDF file.\"\n",
    "    response = client.generate_content([\n",
    "        {\"mime_type\": \"application/pdf\", \"data\": file_data},\n",
    "        prompt\n",
    "    ],generation_config={\n",
    "            \"max_output_tokens\": 8192  # 최대 출력 토큰 수 설정 (예: 8192 토큰, 약 24,000~32,000자)\n",
    "    })\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 text 파일로 저장되어 있다면 load_text_file 함수를 사용하면 됩니다.\n",
    "def load_text_file(pdf_path):\n",
    "\n",
    "    # text 파일 로드\n",
    "    with open(pdf_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "        text = txt_file.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "txt_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "extracted_text = load_text_file(txt_path)\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청크 분할\n",
    "텍스트를 추출한 뒤에는 검색 정확도를 높이기 위해 조금씩 겹치도록 나눠서 작은 단위로 분할(chunk)합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n자 단위로, 일부가 겹치도록 chunking 합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청크할 텍스트입니다.\n",
    "    n (int): 각 청크의 문자 수입니다.\n",
    "    overlap (int): 청크 간 겹치는 문자 수입니다.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 청크된 텍스트 리스트입니다.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크된 텍스트를 저장할 리스트\n",
    "    \n",
    "    # overlap만큼 겹치도록 text를 n의 길이로 chunking\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store 구축\n",
    "NumPy를 사용하여 간단한 Vecotr store 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용하여 간단한 Vecotr store 구축\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소 초기화\n",
    "        \"\"\"\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목 추가\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, top_k=5):\n",
    "        \"\"\"\n",
    "        시맨틱 서치 수행\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): 쿼리 임베딩 벡터.\n",
    "        k (int): 반환할 결과의 수.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: 텍스트와 메타데이터가 포함된 상위 k개 유사 항목.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # 쿼리 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 유사도에 따라 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(top_k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def create_embeddings(embedding_model, texts, device='cuda', batch_size=16):\n",
    "    \"\"\"\n",
    "    SentenceTransformer 모델을 사용하여 지정된 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        embedding_model: 임베딩을 생성할 SentenceTransformer 모델입니다.\n",
    "        texts (list): 임베딩을 생성할 입력 텍스트 리스트입니다.\n",
    "        device (str): 모델을 실행할 장치 ('cuda' for GPU, 'cpu' for CPU).\n",
    "        batch_size (int): 인코딩을 위한 배치 크기입니다.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 모델에 의해 생성된 임베딩입니다.\n",
    "    \"\"\"\n",
    "    # 모델이 지정된 장치에 있는지 확인합니다.\n",
    "    embedding_model = embedding_model.to(device)\n",
    "    \n",
    "    # 지정된 배치 크기로 임베딩을 생성합니다.\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        device=device,\n",
    "        batch_size=batch_size,  # 메모리 사용량을 줄이기 위해 더 작은 배치 크기를 사용합니다.\n",
    "        show_progress_bar=True  # 인코딩 진행 상태를 모니터링하기 위한 진행 표시줄을 표시합니다.\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# GPU 사용 가능 여부를 확인합니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델을 로드합니다.\n",
    "model = \"BAAI/bge-m3\"\n",
    "embedding_model = SentenceTransformer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSE를 사용한 문서 처리\n",
    "이제 핵심 RSE 기능을 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path, chunk_size=800):\n",
    "    \"\"\"\n",
    "    문서를 전처리하여 RSE 시스템에 적합한 형태 변환\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 파일 경로\n",
    "        chunk_size (int): 각 청크의 크기\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[str], SimpleVectorStore, Dict]: chunk, 벡터 저장소, 문서 정보\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from document...\")\n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    # text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # 텍스트 파일 로드\n",
    "    extracted_text = load_text_file(file_path)\n",
    "    \n",
    "    # 추출된 텍스트를 청크로 나눕니다.\n",
    "    print(\"Chunking text into non-overlapping segments...\")    \n",
    "    chunks = chunk_text(extracted_text, chunk_size, overlap=0)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # 텍스트 청크의 임베딩 생성\n",
    "    print(\"Generating embeddings for chunks...\")    \n",
    "    chunk_embeddings = create_embeddings(embedding_model, chunks, device=device, batch_size=1)\n",
    "    \n",
    "    # 벡터 저장소 생성\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    # 각 청크와 임베딩을 vector store에 저장\n",
    "    metadata = [{\"chunk_index\": i, \"source\": file_path} for i in range(len(chunks))]\n",
    "    vector_store.add_item(chunks, chunk_embeddings, metadata)\n",
    "    \n",
    "    # 원문 구조 추적을 통한 문단 복원\n",
    "    doc_info = {\n",
    "        \"chunks\": chunks,\n",
    "        \"source\": file_path,\n",
    "    }\n",
    "    \n",
    "    return chunks, vector_store, doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSE 핵심 알고리즘: 청크 값 계산 및 최적 세그먼트 찾기\n",
    "이제 문서를 처리하고 청크에 대한 임베딩을 생성하는 데 필요한 함수를 준비했으므로, RSE의 핵심 알고리즘을 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty=0.2):\n",
    "    \"\"\"\n",
    "    관련성과 위치를 결합하여 청크 값 계산\n",
    "    \n",
    "    Args:\n",
    "        query (str): 쿼리 텍스트\n",
    "        chunks (List[str]): 문서 청크 리스트\n",
    "        vector_store (SimpleVectorStore): 청크가 포함된 벡터 저장소\n",
    "        irrelevant_chunk_penalty (float): 관련성이 없는 청크에 대한 패널티\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: 청크 값 리스트\n",
    "    \"\"\"\n",
    "    # 쿼리 임베딩 생성\n",
    "    query_embedding = create_embeddings(embedding_model, [query], device=device, batch_size=1)[0]\n",
    "    \n",
    "    # 유사도 점수가 있는 모든 청크 가져오기\n",
    "    num_chunks = len(chunks)\n",
    "    results = vector_store.similarity_search(query_embedding, top_k=num_chunks)\n",
    "    \n",
    "    # 청크 인덱스와 관련성 점수 매핑\n",
    "    relevance_scores = {result[\"metadata\"][\"chunk_index\"]: result[\"score\"] for result in results}\n",
    "    \n",
    "    # 청크 값 계산 (관련성 점수에서 패널티 적용)\n",
    "    chunk_values = []\n",
    "    for i in range(num_chunks):\n",
    "        # 관련성 점수 가져오기 또는 결과에 없는 경우 0으로 설정\n",
    "        score = relevance_scores.get(i, 0.0)\n",
    "        # 관련성이 없는 청크에 대한 패널티 적용\n",
    "        value = score - irrelevant_chunk_penalty\n",
    "        chunk_values.append(value)\n",
    "    \n",
    "    return chunk_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_segments(chunk_values, max_segment_length=20, total_max_length=30, min_segment_value=0.2):\n",
    "    \"\"\"\n",
    "    문서 내에서 의미 있는 구간을 효과적으로 찾아내기 위해, \n",
    "    최대 부분합(Maximum Sum Subarray) 알고리즘을 응용한 방식으로 핵심 문단 선별\n",
    "    \n",
    "    Args:\n",
    "        chunk_values (List[float]): 각 청크의 값\n",
    "        max_segment_length (int): 단일 세그먼트의 최대 길이\n",
    "        total_max_length (int): 모든 세그먼트에 대한 최대 총 길이\n",
    "        min_segment_value (float): 세그먼트로 간주되기 위한 최소 값\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[int, int]]: 최적 세그먼트의 (start, end) 인덱스 리스트\n",
    "    \"\"\"\n",
    "    print(\"Finding optimal continuous text segments...\")\n",
    "    \n",
    "    best_segments = []\n",
    "    segment_scores = []\n",
    "    total_included_chunks = 0\n",
    "    \n",
    "    # 세그먼트 찾기\n",
    "    while total_included_chunks < total_max_length:\n",
    "        best_score = min_segment_value  # 세그먼트로 간주되기 위한 최소 값  \n",
    "        best_segment = None\n",
    "        \n",
    "        # 가능한 모든 시작 위치 시도\n",
    "        for start in range(len(chunk_values)):\n",
    "            # 이미 선택된 세그먼트에 포함된 경우 건너뜁니다.\n",
    "            if any(start >= s[0] and start < s[1] for s in best_segments):\n",
    "                continue\n",
    "                \n",
    "            # 가능한 모든 세그먼트 길이 시도\n",
    "            for length in range(1, min(max_segment_length, len(chunk_values) - start) + 1):\n",
    "                end = start + length\n",
    "                \n",
    "                # 이미 선택된 세그먼트에 포함된 경우 건너뜁니다.\n",
    "                if any(end > s[0] and end <= s[1] for s in best_segments):\n",
    "                    continue\n",
    "                \n",
    "                # 세그먼트 값을 청크 값의 합으로 계산\n",
    "                segment_value = sum(chunk_values[start:end])\n",
    "                \n",
    "                # 이 세그먼트가 더 좋은 경우 업데이트\n",
    "                if segment_value > best_score:\n",
    "                    best_score = segment_value\n",
    "                    best_segment = (start, end)\n",
    "        \n",
    "        # 좋은 세그먼트를 찾은 경우 추가\n",
    "        if best_segment:\n",
    "            best_segments.append(best_segment)\n",
    "            segment_scores.append(best_score)\n",
    "            total_included_chunks += best_segment[1] - best_segment[0]\n",
    "            print(f\"Found segment {best_segment} with score {best_score:.4f}\")\n",
    "        else:\n",
    "            # 더 좋은 세그먼트를 찾을 수 없음   \n",
    "            break\n",
    "    \n",
    "    # 가독성을 위해 문단을 시작 위치 기준으로 정렬합니다.\n",
    "    best_segments = sorted(best_segments, key=lambda x: x[0])\n",
    "    \n",
    "    return best_segments, segment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing and Using Segments for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_segments(chunks, best_segments):\n",
    "    \"\"\"\n",
    "    청크 인덱스를 기반으로 텍스트 세그먼트 재구성\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[str]): 모든 문서 청크 리스트\n",
    "        best_segments (List[Tuple[int, int]]): 세그먼트의 (start, end) 인덱스 리스트\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: 재구성된 텍스트 세그먼트 리스트\n",
    "    \"\"\"\n",
    "    reconstructed_segments = []  \n",
    "    \n",
    "    for start, end in best_segments:\n",
    "        # 이 세그먼트의 청크를 결합하여 완전한 세그먼트 텍스트 생성\n",
    "        segment_text = \" \".join(chunks[start:end])\n",
    "        # 세그먼트 텍스트와 범위를 reconstructed_segments 리스트에 추가\n",
    "        reconstructed_segments.append({\n",
    "            \"text\": segment_text,\n",
    "            \"segment_range\": (start, end),\n",
    "        })\n",
    "    \n",
    "    return reconstructed_segments  # 재구성된 텍스트 세그먼트 리스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_segments_for_context(segments):\n",
    "    \"\"\"\n",
    "    세그먼트를 LLM에 대한 컨텍스트 문자열로 포맷\n",
    "    \n",
    "    Args:\n",
    "        segments (List[Dict]): 세그먼트 딕셔너리 리스트\n",
    "        \n",
    "    Returns:\n",
    "        str: 포맷팅된 컨텍스트 텍스트\n",
    "    \"\"\"\n",
    "    context = []  # 포맷팅된 컨텍스트를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        # 각 세그먼트에 대한 헤더 생성 (인덱스와 청크 범위)\n",
    "        segment_header = f\"SEGMENT {i+1} (Chunks {segment['segment_range'][0]}-{segment['segment_range'][1]-1}):\"\n",
    "        context.append(segment_header)  # 세그먼트 헤더를 컨텍스트 리스트에 추가\n",
    "        context.append(segment['text'])  # 세그먼트 텍스트를 컨텍스트 리스트에 추가\n",
    "        context.append(\"-\" * 80)  # 구분 선 추가\n",
    "    \n",
    "    # 컨텍스트 목록에 있는 항목들을 두 줄씩 띄워 연결한 뒤, 최종 문자열로 반환합니다.\n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Responses with RSE Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model_name='gpt-4.1-nano'):\n",
    "\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"당신은 제공된 Context에 기반하여 답변하는 AI 어시스턴트입니다. 답변이 컨텍스트에서 직접 도출될 수 없는 경우, 다음 문장을 사용하세요: '해당 질문에 답변할 충분한 정보가 없습니다.'\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please answer the question based only on the context provided above. Be concise and accurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 완성된 RSE Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=0.2):\n",
    "    \"\"\"\n",
    "    RSE를 사용한 RAG 파이프라인 구현\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): 문서 경로\n",
    "        query (str): 사용자 쿼리\n",
    "        chunk_size (int): 청크 크기\n",
    "        irrelevant_chunk_penalty (float): 관련성이 없는 청크에 대한 패널티\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 쿼리, 세그먼트, 응답이 포함된 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # 문서를 처리하여 텍스트 추출, 청크로 나누고 임베딩 생성\n",
    "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
    "    \n",
    "    # 쿼리에 대한 관련성 점수와 청크 값 계산\n",
    "    print(\"\\nCalculating relevance scores and chunk values...\")\n",
    "    chunk_values = calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty)\n",
    "    \n",
    "    # 청크 값에 따라 텍스트 세그먼트 찾기\n",
    "    best_segments, scores = find_best_segments(\n",
    "        chunk_values, \n",
    "        max_segment_length=20, \n",
    "        total_max_length=30, \n",
    "        min_segment_value=0.2\n",
    "    )\n",
    "    \n",
    "    # best 청크에서 텍스트 세그먼트 재구성\n",
    "    print(\"\\nReconstructing text segments from chunks...\")\n",
    "    segments = reconstruct_segments(chunks, best_segments)\n",
    "    \n",
    "    # LLM이 이해할 수 있도록 문단들을 하나의 컨텍스트 문자열로 구성합니다.\n",
    "    context = format_segments_for_context(segments)\n",
    "    \n",
    "    # 컨텍스트를 기반으로 응답 생성\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # 결과를 딕셔너리로 저장\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"segments\": segments,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== FINAL RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard 검색 방식과의 비교\n",
    "RSE와 비교하기 위해 Standard 검색 방식을 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_top_k_retrieval(file_path, query, k=10, chunk_size=800):\n",
    "    \"\"\"\n",
    "    standard RAG 파이프라인 구현\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 문서 경로\n",
    "        query (str): 사용자 쿼리\n",
    "        k (int): 검색할 청크 수\n",
    "        chunk_size (int): 청크 크기\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 쿼리, 청크, 응답이 포함된 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STARTING STANDARD TOP-K RETRIEVAL ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # 문서를 처리하여 텍스트 추출, 청크로 나누고 임베딩 생성\n",
    "    chunks, vector_store, doc_info = process_document(file_path, chunk_size)\n",
    "    \n",
    "    # 쿼리에 대한 임베딩 생성\n",
    "    print(\"Creating query embedding and retrieving chunks...\")\n",
    "    query_embedding = create_embeddings(embedding_model, [query], device=device, batch_size=1)[0]\n",
    "    \n",
    "    # 쿼리 임베딩에 따라 상위 k개의 관련성 높은 청크 검색\n",
    "    results = vector_store.search(query_embedding, top_k=k)\n",
    "    retrieved_chunks = [result[\"text\"] for result in results]\n",
    "    \n",
    "    # 검색된 청크를 컨텍스트 문자열로 포맷\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"CHUNK {i+1}:\\n{chunk}\" \n",
    "        for i, chunk in enumerate(retrieved_chunks)\n",
    "    ])\n",
    "    \n",
    "    # 컨텍스트를 기반으로 응답 생성\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # 결과를 딕셔너리로 컴파일\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"chunks\": retrieved_chunks,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== FINAL RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_methods(file_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    RSE와 표준 RAG 비교\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 문서 경로\n",
    "        query (str): 사용자 쿼리\n",
    "        reference_answer (str, optional): 참조 답변\n",
    "    \"\"\"\n",
    "    print(\"\\n========= EVALUATION =========\\n\")\n",
    "    \n",
    "    # RSE  실행\n",
    "    rse_result = rag_with_rse(file_path, query)\n",
    "    \n",
    "    # Standard RAG 방법 실행\n",
    "    standard_result = standard_top_k_retrieval(file_path, query)\n",
    "    \n",
    "    # 참조 답변이 제공된 경우 응답 평가\n",
    "    if reference_answer:\n",
    "        print(\"\\n=== COMPARING RESULTS ===\")\n",
    "        \n",
    "        # 평가 프롬프트 생성\n",
    "        evaluation_prompt = f\"\"\"\n",
    "            Query: {query}\n",
    "\n",
    "            Reference Answer:\n",
    "            {reference_answer}\n",
    "\n",
    "            Response from Standard Retrieval:\n",
    "            {standard_result[\"response\"]}\n",
    "\n",
    "            Response from Relevant Segment Extraction:\n",
    "            {rse_result[\"response\"]}\n",
    "\n",
    "            Compare these two responses against the reference answer. Which one is:\n",
    "            1. More accurate and comprehensive\n",
    "            2. Better at addressing the user's query\n",
    "            3. Less likely to include irrelevant information\n",
    "\n",
    "            Explain your reasoning for each point.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Evaluating responses against reference answer...\")\n",
    "        \n",
    "        # vudrk\n",
    "        evaluation = client_openai.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an objective evaluator of RAG system responses.\"},\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 평가 결과 출력\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(evaluation.choices[0].message.content)\n",
    "    \n",
    "    # 결과 반환\n",
    "    return {\n",
    "        \"rse_result\": rse_result,\n",
    "        \"standard_result\": standard_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 평가 데이터 로드하기\n",
    "df = pd.read_csv('./data_creation/rag_val_new_post.csv')\n",
    "\n",
    "# 평가 데이터에서 첫 번째 쿼리 추출\n",
    "query = df['query'][0]\n",
    "\n",
    "# 참고(정답) 답변\n",
    "reference_answer = df['generation_gt'][0]\n",
    "\n",
    "# 파일 경로\n",
    "file_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_methods(file_path, query, true_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
