{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking for Enhanced RAG Systems\n",
    "이 노트북에서는 RAG 시스템의 검색 품질을 향상시키기 위한 검색 결과에 대한 리랭킹 기법을 구현합니다.   \n",
    "리랭킹은 초기 검색 이후 한번 더 필터링을 수행하여, 응답 생성에 가장 적합한 콘텐츠가 사용되도록 합니다.\n",
    "\n",
    "### 재정렬의 핵심 개념\n",
    "1. 초기 검색: 기본 유사도 검색을 통해 1차적으로 문서를 빠르게 찾음(정확도는 다소 낮음)\n",
    "2. 문서 점수화: 검색된 각 문서가 쿼리와 얼마나 관련 있는지 평가\n",
    "3. Reordering: 문서의 관련성 점수를 기준으로 순서 재배치\n",
    "4. 선택: 응답 생성에 가장 적합한 문서만 최종적으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정하기\n",
    "필요한 라이브러리를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API 클라이언트 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client_openai = OpenAI(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출하기\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 저는 gemini를 이용해 pdf에서 텍스트를 추출하는 방식을 사용합니다.  \n",
    "만약 txt 형태로 파일이 존재한다면 `load_text_file` 함수를 사용하면됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # API 키 설정\n",
    "    genai.configure(api_key=gemini_API_KEY)\n",
    "    client = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "\n",
    "    # PDF 파일 업로드\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        file_data = file.read()\n",
    "\n",
    "\n",
    "    prompt = \"Extract all text from the provided PDF file.\"\n",
    "    response = client.generate_content([\n",
    "        {\"mime_type\": \"application/pdf\", \"data\": file_data},\n",
    "        prompt\n",
    "    ],generation_config={\n",
    "            \"max_output_tokens\": 8192  # 최대 출력 토큰 수 설정 (예: 8192 토큰, 약 24,000~32,000자)\n",
    "    })\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 text 파일로 저장되어 있다면 load_text_file 함수를 사용하면 됩니다.\n",
    "def load_text_file(pdf_path):\n",
    "\n",
    "    # text 파일 로드\n",
    "    with open(pdf_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "        text = txt_file.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "txt_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "extracted_text = load_text_file(txt_path)\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출하기\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 저는 gemini를 이용해 pdf에서 텍스트를 추출하는 방식을 사용합니다.  \n",
    "만약 txt 형태로 파일이 존재한다면 `load_text_file` 함수를 사용하면됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n자 단위로, 일부가 겹치도록 chunking 합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청크할 텍스트입니다.\n",
    "    n (int): 각 청크의 문자 수입니다.\n",
    "    overlap (int): 청크 간 겹치는 문자 수입니다.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 청크된 텍스트 리스트입니다.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크된 텍스트를 저장할 리스트\n",
    "    \n",
    "    # overlap만큼 겹치도록 text를 n의 길이로 chunking\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store 구축\n",
    "NumPy를 사용하여 간단한 Vecotr store 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용하여 간단한 Vecotr store 구축\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소 초기화\n",
    "        \"\"\"\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목 추가\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        시맨틱 서치 수행\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): 쿼리 임베딩 벡터.\n",
    "        k (int): 반환할 결과의 수.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: 텍스트와 메타데이터가 포함된 상위 k개 유사 항목.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # 쿼리 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 유사도에 따라 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def create_embeddings(embedding_model, texts, device='cuda', batch_size=16):\n",
    "    \"\"\"\n",
    "    SentenceTransformer 모델을 사용하여 지정된 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        embedding_model: 임베딩을 생성할 SentenceTransformer 모델입니다.\n",
    "        texts (list): 임베딩을 생성할 입력 텍스트 리스트입니다.\n",
    "        device (str): 모델을 실행할 장치 ('cuda' for GPU, 'cpu' for CPU).\n",
    "        batch_size (int): 인코딩을 위한 배치 크기입니다.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 모델에 의해 생성된 임베딩입니다.\n",
    "    \"\"\"\n",
    "    # 모델이 지정된 장치에 있는지 확인합니다.\n",
    "    embedding_model = embedding_model.to(device)\n",
    "    \n",
    "    # 지정된 배치 크기로 임베딩을 생성합니다.\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        device=device,\n",
    "        batch_size=batch_size,  # 메모리 사용량을 줄이기 위해 더 작은 배치 크기를 사용합니다.\n",
    "        show_progress_bar=True  # 인코딩 진행 상태를 모니터링하기 위한 진행 표시줄을 표시합니다.\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# GPU 사용 가능 여부를 확인합니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델을 로드합니다.\n",
    "model = \"BAAI/bge-m3\"\n",
    "embedding_model = SentenceTransformer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 처리 파이프라인\n",
    "이제 필요한 함수들과 클래스들을 모두 정의했으니, 본격적으로 문서 처리 과정을 구성해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    RAG를 위한 문서를 전처리합니다.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): 파일 경로\n",
    "    chunk_size (int): 각 청크의 문자 단위 크기\n",
    "    chunk_overlap (int): 청크 간 겹치는 문자 수\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: 문서 청크와 임베딩이 포함된 벡터 저장소\n",
    "    \"\"\"\n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    # print(\"Extracting text from PDF...\")\n",
    "    # extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # 텍스트 파일 로드\n",
    "    extracted_text = load_text_file(file_path)\n",
    "    \n",
    "    # 추출된 텍스트 청킹\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # 청크 임베딩 생성\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(embedding_model, chunks, device=device, batch_size=1)\n",
    "    \n",
    "    # 벡터 저장소 생성\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # 각 청크와 임베딩을 VectorStore에 저장\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": file_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM 기반 Reranking 구현\n",
    "OpenAI API를 사용하여 LLM 기반 Reranking 기능 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_llm(query, results, top_n=3, model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    LLM 관련성 점수를 사용하여 검색 결과를 reranking합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        results (List[Dict]): 초기 검색 결과\n",
    "        top_n (int): reranking 후 반환할 결과의 수\n",
    "        model_name (str): 점수 계산에 사용할 모델\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 재정렬된 결과\n",
    "    \"\"\"\n",
    "    print(f\"Reranking {len(results)} documents...\")  # reranking 할 문서 수 출력\n",
    "    \n",
    "    scored_results = []  # 점수가 매겨진 결과를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"You are an expert at evaluating document relevance for search queries.\n",
    "Your task is to rate documents on a scale from 0 to 10 based on how well they answer the given query.\n",
    "\n",
    "Guidelines:\n",
    "- Score 0-2: Document is completely irrelevant\n",
    "- Score 3-5: Document has some relevant information but doesn't directly answer the query\n",
    "- Score 6-8: Document is relevant and partially answers the query\n",
    "- Score 9-10: Document is highly relevant and directly answers the query\n",
    "\n",
    "You MUST respond with ONLY a single integer score between 0 and 10. Do not include ANY other text.\"\"\"\n",
    "    \n",
    "    # 초기 검색 결과를 Context로 활용해 하나의 prompt 생성\n",
    "    for i, result in enumerate(results):\n",
    "        # 진행 상태 출력\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Scoring document {i+1}/{len(results)}...\")\n",
    "        \n",
    "        # user prompt\n",
    "        user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "Document:\n",
    "{result['text']}\n",
    "\n",
    "Rate this document's relevance to the query on a scale from 0 to 10:\"\"\"\n",
    "        \n",
    "        # LLM으로 각 청크별 점수 평가\n",
    "        response = client_openai.chat.completions.create(\n",
    "            model=model_name,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 점수 추출\n",
    "        score_text = response.choices[0].message.content.strip()\n",
    "        score_match = re.search(r'\\b(10|[0-9])\\b', score_text)\n",
    "\n",
    "        if score_match:\n",
    "            score = float(score_match.group(1))\n",
    "        else:\n",
    "            # 점수 추출 실패 시 유사도 점수 그대로 사용\n",
    "            print(f\"Warning: Could not extract score from response: '{score_text}', using similarity score instead\")\n",
    "            score = result[\"similarity\"] * 10\n",
    "        \n",
    "        # 점수가 매겨진 결과를 리스트에 추가\n",
    "        scored_results.append({\n",
    "            \"text\": result[\"text\"],\n",
    "            \"metadata\": result[\"metadata\"],\n",
    "            \"similarity\": result[\"similarity\"],\n",
    "            \"relevance_score\": score\n",
    "        })\n",
    "    \n",
    "    # 유사도(관련성) 점수로 정렬 (내림차순)\n",
    "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # 상위 top_n 결과 반환\n",
    "    return reranked_results[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 Keyword-based Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_keywords(query, results, top_n=3):\n",
    "    \"\"\"\n",
    "    키워드 일치와 위치를 기반으로 하는 간단한 대안 재정렬 방법\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        results (List[Dict]): 초기 검색 결과\n",
    "        top_n (int): rerank 후 반환할 결과의 수\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Reranked results\n",
    "    \"\"\"\n",
    "    # 쿼리에서 중요한 키워드 추출\n",
    "    keywords = [word.lower() for word in query.split() if len(word) > 3]\n",
    "    \n",
    "    scored_results = []  \n",
    "    \n",
    "    for result in results:\n",
    "        document_text = result[\"text\"].lower()  # 문서 텍스트를 소문자로 변환\n",
    "        \n",
    "        # 기본 점수는 벡터 유사도로 시작\n",
    "        base_score = result[\"similarity\"] * 0.5\n",
    "        \n",
    "        # 키워드 점수 초기화\n",
    "        keyword_score = 0\n",
    "        for keyword in keywords:\n",
    "            if keyword in document_text:\n",
    "                # 찾은 키워드에 대해 점수 추가\n",
    "                keyword_score += 0.1\n",
    "                \n",
    "                # 키워드가 처음 나타나는 위치에 더 많은 점수 추가\n",
    "                first_position = document_text.find(keyword)\n",
    "                if first_position < len(document_text) / 4:  # 텍스트의 첫 번째 사분의 위치에 더 많은 점수 추가\n",
    "                    keyword_score += 0.1\n",
    "                \n",
    "                # 키워드 빈도에 대해 점수 추가\n",
    "                frequency = document_text.count(keyword)\n",
    "                keyword_score += min(0.05 * frequency, 0.2)  # 최대 0.2로 제한\n",
    "        \n",
    "        # 기본 점수와 키워드 점수를 결합하여 최종 점수 계산\n",
    "        final_score = base_score + keyword_score\n",
    "        \n",
    "        # 점수가 매겨진 결과를 리스트에 추가\n",
    "        scored_results.append({\n",
    "            \"text\": result[\"text\"],\n",
    "            \"metadata\": result[\"metadata\"],\n",
    "            \"similarity\": result[\"similarity\"],\n",
    "            \"relevance_score\": final_score\n",
    "        })\n",
    "    \n",
    "    # 최종 관련성 점수로 정렬 (내림차순)\n",
    "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # 상위 top_n 결과 반환\n",
    "    return reranked_results[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색된 청크를 기반으로 response 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model_name='gpt-4.1-nano'):\n",
    "\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"당신은 제공된 Context에 기반하여 답변하는 AI 어시스턴트입니다. 답변이 컨텍스트에서 직접 도출될 수 없는 경우, 다음 문장을 사용하세요: '해당 질문에 답변할 충분한 정보가 없습니다.'\"\n",
    "    \n",
    "    # user 프롬프트\n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please answer the question based only on the context provided above. Be concise and accurate.\n",
    "    \"\"\"\n",
    "    # 응답 생성\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full RAG Pipeline with Reranking\n",
    "지금까지 문서 처리, 질문 응답, Reranking을 포함한 RAG 파이프라인의 핵심 구성 요소를 구현했습니다.  \n",
    "이제 이러한 구성 요소를 결합하여 완전한 RAG 파이프라인을 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_reranking(query, vector_store, reranking_method=\"llm\", top_n=3, model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    Reranking을 포함한 RAG 파이프라인\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        reranking_method (str): 재정렬 방법 ('llm' 또는 'keywords')\n",
    "        top_n (int): rerank 후 반환할 결과의 수\n",
    "        model (str): Model for response generation\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 쿼리, 컨텍스트, 응답이 포함된 결과\n",
    "    \"\"\"\n",
    "    # 쿼리 임베딩 생성\n",
    "    query_embedding = create_embeddings(embedding_model, [query], device=device, batch_size=1)[0]\n",
    "    \n",
    "    # 초기 검색 (top_n보다 높은 k 설정)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=10)\n",
    "    \n",
    "    # reranking 적용\n",
    "    if reranking_method == \"llm\":\n",
    "        reranked_results = rerank_with_llm(query, initial_results, top_n=top_n)\n",
    "    elif reranking_method == \"keywords\":\n",
    "        reranked_results = rerank_with_keywords(query, initial_results, top_n=top_n)\n",
    "    else:\n",
    "        # reranking 없이 초기 검색 결과 사용\n",
    "        reranked_results = initial_results[:top_n]\n",
    "    \n",
    "    # reranking 결과의 컨텍스트 결합\n",
    "    context = \"\\n\\n===\\n\\n\".join([result[\"text\"] for result in reranked_results])\n",
    "    \n",
    "    # 컨텍스트를 기반으로 응답 생성\n",
    "    response = generate_response(query, context, model_name)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"reranking_method\": reranking_method,\n",
    "        \"initial_results\": initial_results[:top_n],\n",
    "        \"reranked_results\": reranked_results,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking 품질 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 평가 데이터 로드하기\n",
    "df = pd.read_csv('./data_creation/rag_val_new_post.csv')\n",
    "\n",
    "# 평가 데이터에서 첫 번째 쿼리 추출\n",
    "query = df['query'][0]\n",
    "\n",
    "# 참고(정답) 답변\n",
    "reference_answer = df['generation_gt'][0]\n",
    "\n",
    "# 파일 경로\n",
    "file_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process document\n",
    "vector_store = process_document(file_path)\n",
    "\n",
    "# 각각 방법 비교\n",
    "print(\"Comparing retrieval methods...\")\n",
    "\n",
    "# 1. Standard retrieval (no reranking)\n",
    "print(\"\\n=== STANDARD RETRIEVAL ===\")\n",
    "standard_results = rag_with_reranking(query, vector_store, reranking_method=\"none\")\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"\\nResponse:\\n{standard_results['response']}\")\n",
    "\n",
    "# 2. LLM-based reranking\n",
    "print(\"\\n=== LLM-BASED RERANKING ===\")\n",
    "llm_results = rag_with_reranking(query, vector_store, reranking_method=\"llm\")\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"\\nResponse:\\n{llm_results['response']}\")\n",
    "\n",
    "# 3. Keyword-based reranking\n",
    "print(\"\\n=== KEYWORD-BASED RERANKING ===\")\n",
    "keyword_results = rag_with_reranking(query, vector_store, reranking_method=\"keywords\")\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"\\nResponse:\\n{keyword_results['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reranking(query, standard_results, reranked_results, reference_answer=None):\n",
    "    \"\"\"\n",
    "    reranking 결과와 기본 검색 결과를 비교하여 최종 생성된 응답 품질 평가\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        standard_results (Dict): 기본 검색 결과\n",
    "        reranked_results (Dict): 재정렬된 결과\n",
    "        reference_answer (str, optional): 비교를 위한 기준 답변\n",
    "        \n",
    "    Returns:\n",
    "        str: Evaluation output\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems.\n",
    "    Compare the retrieved contexts and responses from two different retrieval methods.\n",
    "    Assess which one provides better context and a more accurate, comprehensive answer.\"\"\"\n",
    "    \n",
    "    # truncated 된 컨텍스트와 응답을 포함한 비교 텍스트 준비\n",
    "    comparison_text = f\"\"\"Query: {query}\n",
    "\n",
    "Standard Retrieval Context:\n",
    "{standard_results['context'][:1000]}... [truncated]\n",
    "\n",
    "Standard Retrieval Answer:\n",
    "{standard_results['response']}\n",
    "\n",
    "Reranked Retrieval Context:\n",
    "{reranked_results['context'][:1000]}... [truncated]\n",
    "\n",
    "Reranked Retrieval Answer:\n",
    "{reranked_results['response']}\"\"\"\n",
    "\n",
    "    # 정답(참조) 답변이 제공된 경우 비교 텍스트에 포함\n",
    "    if reference_answer:\n",
    "        comparison_text += f\"\"\"\n",
    "        \n",
    "Reference Answer:\n",
    "{reference_answer}\"\"\"\n",
    "\n",
    "    # user 프롬프트\n",
    "    user_prompt = f\"\"\"\n",
    "{comparison_text}\n",
    "\n",
    "Please evaluate which retrieval method provided:\n",
    "1. More relevant context\n",
    "2. More accurate answer\n",
    "3. More comprehensive answer\n",
    "4. Better overall performance\n",
    "\n",
    "Provide a detailed analysis with specific examples.\n",
    "\"\"\"\n",
    "    \n",
    "    # 응답 평가\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model='gpt-4.1-mini',\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 평가 결과 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_answer과 비교하여 reranking 성능 평가\n",
    "evaluation = evaluate_reranking(\n",
    "    query=query,  # 사용자 쿼리\n",
    "    standard_results=standard_results,  # 기본 검색 결과\n",
    "    reranked_results=llm_results,  # LLM 기반 재정렬 결과\n",
    "    reference_answer=reference_answer  # 비교를 위한 기준 답변\n",
    ")\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
