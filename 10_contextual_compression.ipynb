{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# RAG 성능 향상을 위한 문맥 압축 기법\n",
    "이 노트북에서는 RAG 시스템의 효율을 높이기 위한 문맥 기반 압축(contextual compression) 기법을 구현합니다.    \n",
    "검색된 텍스트 청크에서 핵심적인 부분만 남기고 불필요한 정보를 줄여, 응답 품질을 높이고 잡음을 최소화하는 것이 목표입니다.  \n",
    "\n",
    "RAG에서 문서를 불러오면, 관련 정보와 무관한 내용이 섞인 청크가 함께 들어오는 경우가 많습니다. 문맥 압축은 다음과 같은 효과를 제공합니다.\n",
    "\n",
    "- 관련 없는 문장이나 문단 제거\n",
    "- 질의와 관련된 정보에만 집중\n",
    "- 제한된 컨텍스트 창 안에 유용한 정보 밀도 최대화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정하기\n",
    "필요한 라이브러리를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API 클라이언트 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client_openai = OpenAI(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출하기\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 저는 gemini를 이용해 pdf에서 텍스트를 추출하는 방식을 사용합니다.  \n",
    "만약 txt 형태로 파일이 존재한다면 `load_text_file` 함수를 사용하면됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # API 키 설정\n",
    "    genai.configure(api_key=gemini_API_KEY)\n",
    "    client = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "\n",
    "    # PDF 파일 업로드\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        file_data = file.read()\n",
    "\n",
    "\n",
    "    prompt = \"Extract all text from the provided PDF file.\"\n",
    "    response = client.generate_content([\n",
    "        {\"mime_type\": \"application/pdf\", \"data\": file_data},\n",
    "        prompt\n",
    "    ],generation_config={\n",
    "            \"max_output_tokens\": 8192  # 최대 출력 토큰 수 설정 (예: 8192 토큰, 약 24,000~32,000자)\n",
    "    })\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 text 파일로 저장되어 있다면 load_text_file 함수를 사용하면 됩니다.\n",
    "def load_text_file(pdf_path):\n",
    "\n",
    "    # text 파일 로드\n",
    "    with open(pdf_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "        text = txt_file.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "txt_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "extracted_text = load_text_file(txt_path)\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청크 분할\n",
    "텍스트를 추출한 뒤에는 검색 정확도를 높이기 위해 조금씩 겹치도록 나눠서 작은 단위로 분할(chunk)합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 겹치는 n개의 문자 세그먼트로 청크합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청크할 텍스트입니다.\n",
    "    n (int): 각 청크의 문자 수입니다.\n",
    "    overlap (int): 청크 간 겹치는 문자 수입니다.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 청크된 텍스트 리스트입니다.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크된 텍스트를 저장할 빈 리스트를 초기화합니다.\n",
    "    \n",
    "    # (n - overlap) 단계로 텍스트를 반복합니다.\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 인덱스 i부터 i + n까지의 텍스트를 청크 리스트에 추가합니다.\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store 구축\n",
    "NumPy를 사용하여 간단한 Vecotr store 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용하여 간단한 Vecotr store 구축\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소 초기화\n",
    "        \"\"\"\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목 추가\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        시맨틱 서치 수행\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): 쿼리 임베딩 벡터.\n",
    "        k (int): 반환할 결과의 수.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: 텍스트와 메타데이터가 포함된 상위 k개 유사 항목.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # 쿼리 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 유사도에 따라 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def create_embeddings(embedding_model, texts, device='cuda', batch_size=16):\n",
    "    \"\"\"\n",
    "    SentenceTransformer 모델을 사용하여 지정된 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        embedding_model: 임베딩을 생성할 SentenceTransformer 모델입니다.\n",
    "        texts (list): 임베딩을 생성할 입력 텍스트 리스트입니다.\n",
    "        device (str): 모델을 실행할 장치 ('cuda' for GPU, 'cpu' for CPU).\n",
    "        batch_size (int): 인코딩을 위한 배치 크기입니다.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 모델에 의해 생성된 임베딩입니다.\n",
    "    \"\"\"\n",
    "    # 모델이 지정된 장치에 있는지 확인합니다.\n",
    "    embedding_model = embedding_model.to(device)\n",
    "    \n",
    "    # 지정된 배치 크기로 임베딩을 생성합니다.\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        device=device,\n",
    "        batch_size=batch_size,  # 메모리 사용량을 줄이기 위해 더 작은 배치 크기를 사용합니다.\n",
    "        show_progress_bar=True  # 인코딩 진행 상태를 모니터링하기 위한 진행 표시줄을 표시합니다.\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# GPU 사용 가능 여부를 확인합니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델을 로드합니다.\n",
    "model = \"BAAI/bge-m3\"\n",
    "embedding_model = SentenceTransformer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 처리 파이프라인\n",
    "필요한 함수와 클래스를 정의했으니, 이제 문서 처리 파이프라인을 정의해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): 파일 경로\n",
    "    chunk_size (int): 각 청크의 크기(문자)\n",
    "    chunk_overlap (int): 청크 간 중복 범위(문자)\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: 문서 청크와 임베딩이 포함된 벡터 저장소\n",
    "    \"\"\"\n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    # print(\"Extracting text from PDF...\")\n",
    "    # extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # 텍스트 파일 로드\n",
    "    extracted_text = load_text_file(file_path)\n",
    "    \n",
    "    # 추출된 텍스트를 청크로 분할\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # 각 텍스트 청크에 대한 임베딩 생성\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(embedding_model, chunks, device=device, batch_size=1)\n",
    "    \n",
    "    # 벡터 저장소 생성\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # 각 청크와 해당 임베딩을 벡터 저장소에 추가\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": file_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## contextual compression 구현하기\n",
    "이 부분은 이번 노트북의 핵심 부분입니다. LLM을 사용해서 검색된 콘텐츠를 필터링하고 압축(compression)합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_chunk(chunk, query, compression_type=\"selective\", model=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    chunk를 압축하여 쿼리와 관련된 부분만 남기는 함수\n",
    "    \n",
    "    Args:\n",
    "        chunk (str): 압축할 텍스트 청크\n",
    "        query (str): 사용자 쿼리\n",
    "        compression_type (str): 압축 유형 (\"selective\", \"summary\", or \"extraction\")\n",
    "        model (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: Compressed chunk\n",
    "    \"\"\"\n",
    "    # 여러가지 압축 방법에 대한 시스템 프롬프트 정의\n",
    "    if compression_type == \"selective\":\n",
    "        system_prompt = \"\"\"You are an expert at information filtering. \n",
    "        Your task is to analyze a document chunk and extract ONLY the sentences or paragraphs that are directly \n",
    "        relevant to the user's query. Remove all irrelevant content.\n",
    "\n",
    "        Your output should:\n",
    "        1. ONLY include text that helps answer the query\n",
    "        2. Preserve the exact wording of relevant sentences (do not paraphrase)\n",
    "        3. Maintain the original order of the text\n",
    "        4. Include ALL relevant content, even if it seems redundant\n",
    "        5. EXCLUDE any text that isn't relevant to the query\n",
    "\n",
    "        Format your response as plain text with no additional comments.\"\"\"\n",
    "    elif compression_type == \"summary\":\n",
    "        system_prompt = \"\"\"You are an expert at summarization. \n",
    "        Your task is to create a concise summary of the provided chunk that focuses ONLY on \n",
    "        information relevant to the user's query.\n",
    "\n",
    "        Your output should:\n",
    "        1. Be brief but comprehensive regarding query-relevant information\n",
    "        2. Focus exclusively on information related to the query\n",
    "        3. Omit irrelevant details\n",
    "        4. Be written in a neutral, factual tone\n",
    "\n",
    "        Format your response as plain text with no additional comments.\"\"\"\n",
    "    else:  # extraction \n",
    "        system_prompt = \"\"\"You are an expert at information extraction.\n",
    "        Your task is to extract ONLY the exact sentences from the document chunk that contain information relevant \n",
    "        to answering the user's query.\n",
    "\n",
    "        Your output should:\n",
    "        1. Include ONLY direct quotes of relevant sentences from the original text\n",
    "        2. Preserve the original wording (do not modify the text)\n",
    "        3. Include ONLY sentences that directly relate to the query\n",
    "        4. Separate extracted sentences with newlines\n",
    "        5. Do not add any commentary or additional text\n",
    "\n",
    "        Format your response as plain text with no additional comments.\"\"\"\n",
    "\n",
    "    # user prompt\n",
    "    user_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "\n",
    "        Document Chunk:\n",
    "        {chunk}\n",
    "\n",
    "        Extract only the content relevant to answering this query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 응답 생성\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 압축된 chunk\n",
    "    compressed_chunk = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 압축 비율 계산\n",
    "    original_length = len(chunk)\n",
    "    compressed_length = len(compressed_chunk)\n",
    "    compression_ratio = (original_length - compressed_length) / original_length * 100\n",
    "    \n",
    "    return compressed_chunk, compression_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch compressed 구현\n",
    "효율성을 위해 가능하면 여러개의 chunk를 한 번에 압축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_compress_chunks(chunks, query, compression_type=\"selective\", model=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    여러 청크를 개별적으로 압축하는 함수\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[str]): 압축할 텍스트 청크 리스트\n",
    "        query (str): 사용자 쿼리\n",
    "        compression_type (str): 압축 유형 (\"selective\", \"summary\", or \"extraction\")\n",
    "        model (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: 압축 비율이 포함된 압축된 청크 리스트\n",
    "    \"\"\"\n",
    "    print(f\"Compressing {len(chunks)} chunks...\") # 한번에 압축할 청크 수 출력\n",
    "    results = []  # 결과를 저장할 빈 리스트 초기화\n",
    "    total_original_length = 0  # 청크의 총 원래 길이를 저장할 변수 초기화\n",
    "    total_compressed_length = 0  # 청크의 총 압축된 길이를 저장할 변수 초기화\n",
    "    \n",
    "    # 각 청크에 대해 반복\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Compressing chunk {i+1}/{len(chunks)}...\")  # 압축 진행 상태 출력\n",
    "        # 청크를 압축하고 압축된 청크와 압축 비율을 반환\n",
    "        compressed_chunk, compression_ratio = compress_chunk(chunk, query, compression_type, model)\n",
    "        results.append((compressed_chunk, compression_ratio))  \n",
    "        \n",
    "        total_original_length += len(chunk)  \n",
    "        total_compressed_length += len(compressed_chunk)  \n",
    "    \n",
    "    # 전체 압축 비율 계산\n",
    "    overall_ratio = (total_original_length - total_compressed_length) / total_original_length * 100\n",
    "    print(f\"Overall compression ratio: {overall_ratio:.2f}%\")  \n",
    "    \n",
    "    return results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model_name=\"gpt-4.1-nano\"):\n",
    "    \"\"\"\n",
    "    쿼리와 컨텍스트를 기반으로 응답을 생성하는 함수\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        context (str): 압축된 청크의 컨텍스트 텍스트\n",
    "        model (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based only on the provided context.\n",
    "    If you cannot find the answer in the context, state that you don't have enough information.\"\"\"\n",
    "            \n",
    "    # user 프롬프트\n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please provide a comprehensive answer based only on the context above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 응답 생성\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Compression를 포함한 RAG Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_compression(file_path, query, k=10, compression_type=\"selective\", model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    RAG 파이프라인에 Contextual Compression 적용\n",
    "\n",
    "    Args:\n",
    "        file_path (str): 파일 경로\n",
    "        query (str): 사용자 쿼리\n",
    "        k (int): 초기 검색할 청크 수\n",
    "        compression_type (str): 압축 유형\n",
    "        model_name (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        dict: 쿼리, 압축된 청크, 응답이 포함된 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RAG WITH CONTEXTUAL COMPRESSION ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Compression type: {compression_type}\")\n",
    "    \n",
    "    # RAG에 사용할 문서에서 텍스트를 추출하고 청킹 후 임베딩 생성\n",
    "    vector_store = process_document(file_path)\n",
    "    \n",
    "    # 쿼리에 대한 임베딩 생성\n",
    "    query_embedding = create_embeddings(embedding_model, [query], device=device, batch_size=1)\n",
    "    \n",
    "    # 쿼리 임베딩을 기반으로 가장 유사한 청크 검색\n",
    "    print(f\"Retrieving top {k} chunks...\")\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    retrieved_chunks = [result[\"text\"] for result in results]\n",
    "    \n",
    "    # 검색된 청크에 대해 압축 적용\n",
    "    compressed_results = batch_compress_chunks(retrieved_chunks, query, compression_type, model_name)\n",
    "    compressed_chunks = [result[0] for result in compressed_results]\n",
    "    compression_ratios = [result[1] for result in compressed_results]\n",
    "    \n",
    "    # 빈 문자열로 압축된 청크 제거\n",
    "    filtered_chunks = [(chunk, ratio) for chunk, ratio in zip(compressed_chunks, compression_ratios) if chunk.strip()]\n",
    "    \n",
    "    if not filtered_chunks:\n",
    "        # 모든 청크가 빈 문자열로 압축된 경우 원본 청크 사용\n",
    "        print(\"Warning: All chunks were compressed to empty strings. Using original chunks.\")\n",
    "        filtered_chunks = [(chunk, 0.0) for chunk in retrieved_chunks]\n",
    "    else:\n",
    "        compressed_chunks, compression_ratios = zip(*filtered_chunks)\n",
    "    \n",
    "    # 압축된 청크에서 컨텍스트 생성\n",
    "    context = \"\\n\\n---\\n\\n\".join(compressed_chunks)\n",
    "    \n",
    "    # 압축된 청크를 기반으로 응답 생성\n",
    "    print(\"Generating response based on compressed chunks...\")\n",
    "    response = generate_response(query, context, model_name)\n",
    "    \n",
    "    # 딕셔너리 형태로 저장\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"original_chunks\": retrieved_chunks,\n",
    "        \"compressed_chunks\": compressed_chunks,\n",
    "        \"compression_ratios\": compression_ratios,\n",
    "        \"context_length_reduction\": f\"{sum(compression_ratios)/len(compression_ratios):.2f}%\",\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression 여부에 따른 RAG 비교\n",
    "일반 RAG와 Context Compression 기능이 추가된 버전을 비교하는 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_rag(file_path, query, k=10, model_name=\"gpt-4.1-nano\"):\n",
    "    \"\"\"\n",
    "    Standard RAG without compression.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 파일 경로\n",
    "        query (str): 사용자 쿼리\n",
    "        k (int): 검색할 청크 수\n",
    "        model_name (str): 사용할 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        dict: 쿼리, 청크, 응답이 포함된 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STANDARD RAG ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # RAG에 사용할 문서에서 텍스트를 추출하고 청킹 후 임베딩 생성\n",
    "    vector_store = process_document(file_path)\n",
    "\n",
    "    \n",
    "    # 쿼리에 대한 임베딩 생성\n",
    "    query_embedding = create_embeddings(embedding_model, [query], device=device, batch_size=1)\n",
    "    \n",
    "    # 쿼리 임베딩을 기반으로 가장 유사한 청크 검색\n",
    "    print(f\"Retrieving top {k} chunks...\")\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    retrieved_chunks = [result[\"text\"] for result in results]\n",
    "    \n",
    "    # 검색된 청크에서 컨텍스트 생성\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    # 검색된 청크를 기반으로 응답 생성\n",
    "    print(\"Generating response...\")\n",
    "    response = generate_response(query, context, model_name)\n",
    "    \n",
    "    # 결과 딕셔너리 준비\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"chunks\": retrieved_chunks,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(query, responses, reference_answer):\n",
    "    \"\"\"\n",
    "    reference answer를 기준으로 여러 응답을 평가하는 함수\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        responses (Dict[str, str]): 방법별 응답 딕셔너리\n",
    "        reference_answer (str): 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        str: 평가 텍스트\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트 \n",
    "    system_prompt = \"\"\"You are an objective evaluator of RAG responses. Compare different responses to the same query\n",
    "    and determine which is most accurate, comprehensive, and relevant to the query.\"\"\"\n",
    "    \n",
    "    # 사용자 프롬프트 정의\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "\n",
    "    Reference Answer: {reference_answer}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    for method, response in responses.items():\n",
    "        user_prompt += f\"\\n{method.capitalize()} Response:\\n{response}\\n\"\n",
    "    \n",
    "    # 평가 기준을 사용자 프롬프트에 추가\n",
    "    user_prompt += \"\"\"\n",
    "    Please evaluate these responses based on:\n",
    "    1. Factual accuracy compared to the reference\n",
    "    2. Comprehensiveness - how completely they answer the query\n",
    "    3. Conciseness - whether they avoid irrelevant information\n",
    "    4. Overall quality\n",
    "\n",
    "    Rank the responses from best to worst with detailed explanations.\n",
    "    \"\"\"\n",
    "\n",
    "    #### 여기까지 사용자 프롬프트 정의 과정 ####\n",
    "    \n",
    "    # 평가 응답 생성\n",
    "    evaluation_response = client_openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return evaluation_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_compression(file_path, query, reference_answer=None, compression_types=[\"selective\", \"summary\", \"extraction\"]):\n",
    "    \"\"\"\n",
    "    Contextual Compression 기술을 Standard RAG와 비교하는 함수\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 파일 경로\n",
    "        query (str): 사용자 쿼리\n",
    "        reference_answer (str): 선택적 참조 답변\n",
    "        compression_types (List[str]): 평가할 압축 유형\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EVALUATING CONTEXTUAL COMPRESSION ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # 압축 없이 표준 RAG 실행\n",
    "    standard_result = standard_rag(file_path, query)\n",
    "    \n",
    "    compression_results = {}\n",
    "    \n",
    "    # 각각의 Contextual Compression 기술에 대해 RAG 실행\n",
    "    for comp_type in compression_types:\n",
    "        print(f\"\\nTesting {comp_type} compression...\")\n",
    "        compression_results[comp_type] = rag_with_compression(file_path, query, compression_type=comp_type)\n",
    "    \n",
    "    # 평가를 위한 응답 생성\n",
    "    responses = {\n",
    "        \"standard\": standard_result[\"response\"]\n",
    "    }\n",
    "    for comp_type in compression_types:\n",
    "        responses[comp_type] = compression_results[comp_type][\"response\"]\n",
    "    \n",
    "    # 정답(참조) 답변이 제공된 경우 응답 평가\n",
    "    if reference_answer:\n",
    "        evaluation = evaluate_responses(query, responses, reference_answer)\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(evaluation)\n",
    "    else:\n",
    "        evaluation = \"No reference answer provided for evaluation.\"\n",
    "    \n",
    "    # 각각의 Contextual Compression 기술을 이용하여 생성한 답변에 대한 점수 계산\n",
    "    metrics = {}\n",
    "    for comp_type in compression_types:\n",
    "        metrics[comp_type] = {\n",
    "            \"avg_compression_ratio\": f\"{sum(compression_results[comp_type]['compression_ratios'])/len(compression_results[comp_type]['compression_ratios']):.2f}%\",\n",
    "            \"total_context_length\": len(\"\\n\\n\".join(compression_results[comp_type]['compressed_chunks'])),\n",
    "            \"original_context_length\": len(\"\\n\\n\".join(standard_result['chunks']))\n",
    "        }\n",
    "    \n",
    "    # 평가 결과, 응답, 점수 반환\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"responses\": responses,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"metrics\": metrics,\n",
    "        \"standard_result\": standard_result,\n",
    "        \"compression_results\": compression_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Our Complete System (Custom Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 평가 데이터 로드하기\n",
    "df = pd.read_csv('./data_creation/rag_val_new_post.csv')\n",
    "\n",
    "# 평가 데이터에서 첫 번째 쿼리 추출\n",
    "query = df['query'][0]\n",
    "\n",
    "# 참고(정답) 답변\n",
    "reference_answer = df['generation_gt'][0]\n",
    "\n",
    "# 파일 경로\n",
    "file_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "file_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "# Run evaluation with different compression techniques  \n",
    "# Compression types:  \n",
    "# - \"selective\": Retains key details while omitting less relevant parts  \n",
    "# - \"summary\": Provides a concise version of the information  \n",
    "# - \"extraction\": Extracts relevant sentences verbatim from the document  \n",
    "results = evaluate_compression(  \n",
    "    file_path=file_path,  \n",
    "    query=query,  \n",
    "    reference_answer=reference_answer,  \n",
    "    compression_types=[\"selective\", \"summary\", \"extraction\"]  \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
