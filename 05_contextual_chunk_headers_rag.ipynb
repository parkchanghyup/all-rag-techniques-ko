{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Simple RAG에서의 Contextual Chunk Headers (CCH)\n",
    "\n",
    "RAG는 응답을 생성하기 전에 **외부의 관련 지식**을 검색하여, 언어 모델의 사실 기반 정확도를 높이는 방식입니다. 하지만 일반적인 chunking 방식은 중요한 문맥 정보가 사라지는 경우가 많아, 검색 정확도에 한계가 생길 수 있습니다.\n",
    "\n",
    "이를 보완하기 위해 **Contextual Chunk Headers(CCH)** 기법을 도입하였습니다. 이 방식은 각 chunk 앞에 **문서 제목이나 섹션 제목과 같은 상위 문맥 정보**를 붙여서 임베딩합니다. 이렇게 하면 검색 품질이 향상되며, 문맥에서 벗어난 잘못된 응답도 줄일 수 있습니다.\n",
    "\n",
    "#### 이 노트북의 주요 단계는 다음과 같습니다:\n",
    "\n",
    "1. **데이터 수집(Data Ingestion)**: 텍스트 데이터를 불러오고 전처리합니다.\n",
    "2. **문맥 헤더 기반 Chunking**: 섹션 제목 등을 추출하여 각 chunk 앞에 붙입니다.\n",
    "3. **임베딩 생성(Embedding Creation)**: 문맥이 보강된 chunk들을 숫자 벡터로 변환합니다.\n",
    "4. **의미 기반 검색(Semantic Search)**: 사용자 질의와 관련된 chunk들을 검색합니다.\n",
    "5. **응답 생성(Response Generation)**: 검색된 텍스트를 바탕으로 언어 모델이 응답을 생성합니다.\n",
    "6. **응답 평가(Evaluation)**: 별도의 평가 기준을 활용해 응답의 정확도를 점검합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정하기\n",
    "필요한 라이브러리를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API 클라이언트 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client_openai = OpenAI(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출하기\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 저는 gemini를 이용해 pdf에서 텍스트를 추출하는 방식을 사용합니다.  \n",
    "만약 txt 형태로 파일이 존재한다면 `load_text_file` 함수를 사용하면됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # API 키 설정\n",
    "    genai.configure(api_key=gemini_API_KEY)\n",
    "    client = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "\n",
    "    # PDF 파일 업로드\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        file_data = file.read()\n",
    "\n",
    "\n",
    "    prompt = \"Extract all text from the provided PDF file.\"\n",
    "    response = client.generate_content([\n",
    "        {\"mime_type\": \"application/pdf\", \"data\": file_data},\n",
    "        prompt\n",
    "    ],generation_config={\n",
    "            \"max_output_tokens\": 8192  # 최대 출력 토큰 수 설정 (예: 8192 토큰, 약 24,000~32,000자)\n",
    "    })\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 text 파일로 저장되어 있다면 load_text_file 함수를 사용하면 됩니다.\n",
    "def load_text_file(pdf_path):\n",
    "\n",
    "    # text 파일 로드\n",
    "    with open(pdf_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "        text = txt_file.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "txt_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "extracted_text = load_text_file(txt_path)\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 효율적 검색을 위한 문단별 요약 헤더 생성\n",
    "검색 효율을 높이기 위해, 각 chunk마다 LLM 모델을 사용해 내용을 잘 설명하는 헤더를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_header(chunk, model_name=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트 청크에 대한 간결한 제목을 생성하는 LLM 모델을 활용합니다.\n",
    "\n",
    "    Args:\n",
    "    chunk (str): 제목으로 요약할 텍스트 청크\n",
    "    model (str): 헤더를 생성하는데 사용할 모델. \n",
    "\n",
    "    Returns:\n",
    "    str: 생성된 헤더/제목.\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"주어진 텍스트에 대한 제목을 생성합니다. 제목은 최대 10자 이내로 생성합니다.\"\n",
    "    \n",
    "    # 응답 생성\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": chunk}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 생성된 헤더/제목 반환, 앞/뒤 공백 제거\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_with_headers(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunk text를 작은 청크로 나누고, 각 청크에 대해 LLM 모델을 활용해 헤더를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청크할 텍스트\n",
    "    n (int): 각 청크의 문자 수\n",
    "    overlap (int): 청크 간 중복 문자 수\n",
    "\n",
    "    Returns:\n",
    "    List[dict]: 헤더와 텍스트가 포함된 딕셔너리 리스트\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크된 텍스트를 저장할 리스트\n",
    "\n",
    "    # overlap만큼 겹치도록 text를 n의 길이로 chunking\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunk = text[i:i + n]  # 텍스트 청크 추출\n",
    "        header = generate_chunk_header(chunk)  # LLM 모델을 활용해 헤더 생성\n",
    "        chunks.append({\"header\": header, \"text\": chunk})  # 헤더와 청크를 리스트에 추가\n",
    "\n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청크 생성\n",
    "# 추출한 텍스트를 1000자씩, 200자가 겹치도록 나눕니다.\n",
    "text_chunks = chunk_text_with_headers(extracted_text, 1000, 200)\n",
    "\n",
    "# 첫 번째 청크 출력\n",
    "print(\"Sample Chunk:\")\n",
    "print(\"Header:\", text_chunks[0]['header'])\n",
    "print(\"Content:\", text_chunks[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분할된 청크와 해당 청크의 header의 임베딩 생성\n",
    "임베딩은 텍스트를 숫자 벡터로 변환하여 효율적인 유사도 검색을 가능하게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def create_embeddings(embedding_model, texts, device='cuda', batch_size=16):\n",
    "    \"\"\"\n",
    "    SentenceTransformer 모델을 사용하여 지정된 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        embedding_model: 임베딩을 생성할 SentenceTransformer 모델입니다.\n",
    "        texts (list): 임베딩을 생성할 입력 텍스트 리스트입니다.\n",
    "        device (str): 모델을 실행할 장치 ('cuda' for GPU, 'cpu' for CPU).\n",
    "        batch_size (int): 인코딩을 위한 배치 크기입니다.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 모델에 의해 생성된 임베딩입니다.\n",
    "    \"\"\"\n",
    "    # 모델이 지정된 장치에 있는지 확인합니다.\n",
    "    embedding_model = embedding_model.to(device)\n",
    "    \n",
    "    # 지정된 배치 크기로 임베딩을 생성합니다.\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        device=device,\n",
    "        batch_size=batch_size,  # 메모리 사용량을 줄이기 위해 더 작은 배치 크기를 사용합니다.\n",
    "        show_progress_bar=True  # 인코딩 진행 상태를 모니터링하기 위한 진행 표시줄을 표시합니다.\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# GPU 사용 가능 여부를 확인합니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델을 로드합니다.\n",
    "model = \"BAAI/bge-m3\"\n",
    "embedding_model = SentenceTransformer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "embeddings = [] \n",
    "\n",
    "for chunk in tqdm(text_chunks, desc=\"Generating embeddings\"):\n",
    "    # 텍스트 청크의 임베딩 생성\n",
    "    text_embedding = create_embeddings(embedding_model, [chunk[\"text\"]], device=device, batch_size=1)\n",
    "    # 헤더 청크의 임베딩 생성\n",
    "    header_embedding = create_embeddings(embedding_model, [chunk[\"header\"]], device=device, batch_size=1)\n",
    "    \n",
    "    embeddings.append({\"header\": chunk[\"header\"], \"text\": chunk[\"text\"], \"embedding\": text_embedding, \"header_embedding\": header_embedding})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search\n",
    "코사인 유사도를 구현하여 사용자 쿼리에 가장 관련성이 높은 텍스트 청크를 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    두 벡터 간의 코사인 유사도를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): 첫 번째 벡터입니다.\n",
    "    vec2 (np.ndarray): 두 번째 벡터입니다.\n",
    "\n",
    "    Returns:\n",
    "    float: 두 벡터 간의 코사인 유사도입니다.\n",
    "    \"\"\"\n",
    "    # 두 벡터의 내적을 계산하고 두 벡터의 크기의 곱으로 나눕니다.\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, chunks, k=5):\n",
    "    \"\"\"\n",
    "    쿼리에 대한 가장 관련성 높은 청크를 검색합니다.\n",
    "\n",
    "    Args:\n",
    "    query (str): 사용자 쿼리.\n",
    "    chunks (List[dict]): 임베딩이 포함된 텍스트 청크 리스트.\n",
    "    k (int): 상위 k개 결과.\n",
    "\n",
    "    Returns:\n",
    "    List[dict]: 상위 k개 관련성 높은 청크.\n",
    "    \"\"\"\n",
    "    # 쿼리에 대한 임베딩을 생성\n",
    "    query_embedding = create_embeddings(embedding_model, [query])[0]\n",
    "\n",
    "    similarities = []  \n",
    "    \n",
    "    # 각 청크에 대해 유사도 점수를 계산\n",
    "    for chunk in chunks:\n",
    "        # 쿼리 임베딩과 청크 텍스트 임베딩 간의 코사인 유사도 계산\n",
    "        sim_text = cosine_similarity(np.array(query_embedding), np.array(chunk[\"embedding\"][0]))\n",
    "        # 쿼리 임베딩과 청크 헤더 임베딩 간의 코사인 유사도 계산\n",
    "        sim_header = cosine_similarity(np.array(query_embedding), np.array(chunk[\"header_embedding\"][0]))\n",
    "        # 평균 유사도 점수 계산\n",
    "        avg_similarity = (sim_text + sim_header) / 2\n",
    "        # 청크와 평균 유사도 점수를 리스트에 추가\n",
    "        similarities.append((chunk, avg_similarity))\n",
    "\n",
    "    # 유사도 점수에 따라 청크를 내림차순으로 정렬\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    # 상위 k개 관련성 높은 청크 반환\n",
    "    return [x[0] for x in similarities[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 평가 데이터 로드하기\n",
    "df = pd.read_csv('./data_creation/rag_val_new_post.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 데이터에서 첫 번째 쿼리 추출\n",
    "query = df['query'][0]\n",
    "\n",
    "# 시맨틱 검색 수행\n",
    "top_chunks = semantic_search(query, text_chunks, embeddings, k=3)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Query:\", query)\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Header {i+1}: {chunk['header']}\")\n",
    "    print(f\"Content:\\n{chunk['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색된 청크를 기반으로 response 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_prompt, user_message ,model_name='gpt-4.1-nano'):\n",
    "    \n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},# Define the system prompt for the AI assistant\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    # print(response.choices[0].message.content)\n",
    "\n",
    "    return response\n",
    "\n",
    "# 사용자 프롬프트\n",
    "user_prompt = \"\\n\".join([f\"Header: {chunk['header']}\\nContent:\\n{chunk['text']}\" for chunk in top_chunks])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# 시스템 프롬프트\n",
    "system_prompt = \"당신은 제공된 Context에 기반하여 답변하는 AI 어시스턴트입니다. 답변이 컨텍스트에서 직접 도출될 수 없는 경우, 다음 문장을 사용하세요: '해당 질문에 답변할 충분한 정보가 없습니다.'\"\n",
    "\n",
    "# 응답 생성\n",
    "ai_response = generate_response(system_prompt, user_prompt,'gpt-4.1-nano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성 응답 평가하기\n",
    "생성된 답변과 예상 답변을 비교하여 평가합니다. 답변 평가시에는 LLM을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 시스템 프롬프트\n",
    "evaluate_system_prompt = \"\"\"You are an intelligent evaluation system. \n",
    "Assess the AI assistant's response based on the provided context. \n",
    "- Assign a score of 1 if the response is very close to the true answer. \n",
    "- Assign a score of 0.5 if the response is partially correct. \n",
    "- Assign a score of 0 if the response is incorrect.\n",
    "Return only the score (0, 0.5, or 1).\"\"\"\n",
    "\n",
    "# 참고(정답) 답변\n",
    "true_answer = df['generation_gt'][0]\n",
    "\n",
    "# 평가 프롬프트\n",
    "evaluation_prompt = f\"\"\"\n",
    "User Query: {query}\n",
    "AI Response: {ai_response}\n",
    "True Answer: {true_answer}\n",
    "{evaluate_system_prompt}\n",
    "\"\"\"\n",
    "\n",
    "# 평가\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt, 'gpt-4.1-mini')\n",
    "\n",
    "# 평가 점수 출력\n",
    "print(\"Evaluation Score:\", evaluation_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 추론 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    query = df['query'][i]\n",
    "    top_chunks = semantic_search(query, embeddings, k=3)\n",
    "\n",
    "        \n",
    "    # 사용자 프롬프트\n",
    "    user_prompt = \"\\n\".join([f\"Header: {chunk['header']}\\nContent:\\n{chunk['text']}\" for chunk in top_chunks])\n",
    "    user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "    \n",
    "\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"당신은 제공된 Context에 기반하여 답변하는 AI 어시스턴트입니다. 답변이 컨텍스트에서 직접 도출될 수 없는 경우, 다음 문장을 사용하세요: '해당 질문에 답변할 충분한 정보가 없습니다.'\"\n",
    "    \n",
    "    # 응답 생성\n",
    "    ai_response = generate_response(system_prompt, user_prompt,'gpt-4.1-nano')\n",
    "\n",
    "    print(ai_response.choices[0].message.content)\n",
    "    \n",
    "    # 평가 프롬프트\n",
    "    evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {df['generation_gt'][i]}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "    # 평가 응답 생성\n",
    "    evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt, 'gpt-4.1-mini')\n",
    "\n",
    "    # 평가 응답 출력\n",
    "    print(evaluation_response.choices[0].message.content)\n",
    "    result.append(evaluation_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 점수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "result = [float(x.replace('\\n','').replace('Score: ',''))for x in result]\n",
    "np.average(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
