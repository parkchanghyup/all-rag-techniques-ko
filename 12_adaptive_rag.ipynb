{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# RAG 시스템을 더 똑똑하게: Adaptive RAG\n",
    "이 노트북에서는 질문의 성격에 따라 가장 적합한 검색 전략을 유연하게 선택하는 '적응형 검색 시스템'을 구현합니다. 이를 통해 RAG 시스템은 훨씬 더 정확하고 맥락에 잘 맞는 답변을 제공할 수 있게 됩니다.\n",
    "\n",
    "질문의 종류에 따라, 필요한 정보 접근 방식은 달라질 수밖에 없습니다. 그래서 Adaptive RAG에서는 다음과 같은 과정을 따릅니다:\n",
    "1. 질문이 어떤 유형인지 분류합니다 (사실 기반, 분석적, 의견형, 맥락 중심 등)\n",
    "2. 그에 맞는 검색 전략을 선택합니다.\n",
    "3. 상황에 특화된 검색 기법을 실행합니다.\n",
    "4. 질문에 꼭 맞는 방식으로 답변을 구성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정하기\n",
    "필요한 라이브러리를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API 클라이언트 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client_openai = OpenAI(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출하기\n",
    "RAG를 구현하려면 먼저 텍스트 데이터 소스가 필요합니다. 저는 gemini를 이용해 pdf에서 텍스트를 추출하는 방식을 사용합니다.  \n",
    "만약 txt 형태로 파일이 존재한다면 `load_text_file` 함수를 사용하면됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # API 키 설정\n",
    "    genai.configure(api_key=gemini_API_KEY)\n",
    "    client = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "\n",
    "    # PDF 파일 업로드\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        file_data = file.read()\n",
    "\n",
    "\n",
    "    prompt = \"Extract all text from the provided PDF file.\"\n",
    "    response = client.generate_content([\n",
    "        {\"mime_type\": \"application/pdf\", \"data\": file_data},\n",
    "        prompt\n",
    "    ],generation_config={\n",
    "            \"max_output_tokens\": 8192  # 최대 출력 토큰 수 설정 (예: 8192 토큰, 약 24,000~32,000자)\n",
    "    })\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 text 파일로 저장되어 있다면 load_text_file 함수를 사용하면 됩니다.\n",
    "def load_text_file(pdf_path):\n",
    "\n",
    "    # text 파일 로드\n",
    "    with open(pdf_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "        text = txt_file.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "txt_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "extracted_text = load_text_file(txt_path)\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 청크 분할\n",
    "텍스트를 추출한 뒤에는 검색 정확도를 높이기 위해 조금씩 겹치도록 나눠서 작은 단위로 분할(chunk)합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n자 단위로, 일부가 겹치도록 chunking 합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청크할 텍스트입니다.\n",
    "    n (int): 각 청크의 문자 수입니다.\n",
    "    overlap (int): 청크 간 겹치는 문자 수입니다.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 청크된 텍스트 리스트입니다.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크된 텍스트를 저장할 리스트\n",
    "    \n",
    "    # overlap만큼 겹치도록 text를 n의 길이로 chunking\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store 구축\n",
    "NumPy를 사용하여 간단한 Vecotr store 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용하여 간단한 Vecotr store 구축\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소 초기화\n",
    "        \"\"\"\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목 추가\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        쿼리 임베딩과 가장 유사한 항목 찾기.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): 쿼리 임베딩 벡터.\n",
    "        k (int): 반환할 결과의 수.\n",
    "        filter_func (callable, optional): 결과를 필터링하는 함수.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: 텍스트와 메타데이터가 포함된 상위 k개 유사 항목.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 벡터가 저장되어 있지 않으면 빈 리스트 반환\n",
    "        \n",
    "        # 쿼리 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # 필터가 제공된 경우 적용\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # 코사인 유사도 계산\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # 인덱스와 유사도 점수 추가\n",
    "        \n",
    "        # 유사도(내림차순)로 정렬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # 텍스트 추가\n",
    "                \"metadata\": self.metadata[idx],  # 메타데이터 추가\n",
    "                \"similarity\": score  # 유사도 점수 추가\n",
    "            })\n",
    "        \n",
    "        return results  # 상위 k개 결과 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def create_embeddings(embedding_model, texts, device='cuda', batch_size=16):\n",
    "    \"\"\"\n",
    "    SentenceTransformer 모델을 사용하여 지정된 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        embedding_model: 임베딩을 생성할 SentenceTransformer 모델입니다.\n",
    "        texts (list): 임베딩을 생성할 입력 텍스트 리스트입니다.\n",
    "        device (str): 모델을 실행할 장치 ('cuda' for GPU, 'cpu' for CPU).\n",
    "        batch_size (int): 인코딩을 위한 배치 크기입니다.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 모델에 의해 생성된 임베딩입니다.\n",
    "    \"\"\"\n",
    "    # 모델이 지정된 장치에 있는지 확인합니다.\n",
    "    embedding_model = embedding_model.to(device)\n",
    "    \n",
    "    # 지정된 배치 크기로 임베딩을 생성합니다.\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        device=device,\n",
    "        batch_size=batch_size,  # 메모리 사용량을 줄이기 위해 더 작은 배치 크기를 사용합니다.\n",
    "        show_progress_bar=True  # 인코딩 진행 상태를 모니터링하기 위한 진행 표시줄을 표시합니다.\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# GPU 사용 가능 여부를 확인합니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델을 로드합니다.\n",
    "model = \"BAAI/bge-m3\"\n",
    "embedding_model = SentenceTransformer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    문서를 처리하여 Adaptive RAG에 적합한 형태로 변환합니다.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): 파일 경로\n",
    "    chunk_size (int): 각 청크의 크기\n",
    "    chunk_overlap (int): 청크 간 중복 범위\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], SimpleVectorStore]: 문서 청크와 벡터 저장소.\n",
    "    \"\"\"\n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    # print(\"Extracting text from PDF...\")\n",
    "    # extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # 텍스트 파일 로드\n",
    "    extracted_text = load_text_file(file_path)\n",
    "    \n",
    "    # 추출된 텍스트를 청크로 분할\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # 텍스트 청크의 임베딩 생성\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(embedding_model, chunks, device=device, batch_size=4)\n",
    "    \n",
    "    # 벡터 저장소 생성\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # 각 청크와 임베딩을 VectorStore에 저장\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": file_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    \n",
    "    # 청크와 벡터 저장소 반환\n",
    "    return chunks, store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query(query, model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    쿼리를 다음 네 가지 유형 중 하나로 분류합니다.\n",
    "    - Factual, Analytical, Opinion, or Contextual.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        model_name (str): 사용할 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: Query category\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"You are an expert at classifying questions. \n",
    "        Classify the given query into exactly one of these categories:\n",
    "        - Factual: Queries seeking specific, verifiable information.\n",
    "        - Analytical: Queries requiring comprehensive analysis or explanation.\n",
    "        - Opinion: Queries about subjective matters or seeking diverse viewpoints.\n",
    "        - Contextual: Queries that depend on user-specific context.\n",
    "\n",
    "        Return ONLY the category name, without any explanation or additional text.\n",
    "    \"\"\"\n",
    "\n",
    "    # 사용자 프롬프트\n",
    "    user_prompt = f\"Classify this query: {query}\"\n",
    "    \n",
    "    # 응답 생성\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 분류 결과 추출\n",
    "    category = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 유효한 분류 결과 목록 정의\n",
    "    valid_categories = [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]\n",
    "    \n",
    "    # 생성된 분류 결과가 유효한지 확인\n",
    "    for valid in valid_categories:\n",
    "        if valid in category:\n",
    "            return valid\n",
    "    \n",
    "    # 분류 실패 시 \"Factual\"로 기본값 설정\n",
    "    return \"Factual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Specialized Retrieval Strategies\n",
    "### 1. 사실 기반 전략 – 정확성에 집중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factual_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    정확성에 중점을 둔 사실 기반 쿼리에 대한 검색 전략.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 반환할 문서의 수\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서\n",
    "    \"\"\"\n",
    "    print(f\"Executing Factual retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"You are an expert at enhancing search queries.\n",
    "        Your task is to reformulate the given factual query to make it more precise and \n",
    "        specific for information retrieval. Focus on key entities and their relationships.\n",
    "\n",
    "        Provide ONLY the enhanced query without any explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"Enhance this factual query: {query}\"\n",
    "    \n",
    "    # 응답 생성\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 변환된(향상된) 쿼리 추출 및 출력\n",
    "    enhanced_query = response.choices[0].message.content.strip()\n",
    "    print(f\"Enhanced query: {enhanced_query}\")\n",
    "    \n",
    "    # 임베딩 생성\n",
    "    query_embedding = create_embeddings(embedding_model, [enhanced_query], device=device, batch_size=1)[0]\n",
    "    \n",
    "    # 유사도 검색을 통해 문서 검색\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    ranked_results = []\n",
    "    \n",
    "    # LLM을 사용하여 문서 관련성 점수 계산 및 순위 매기기\n",
    "    for doc in initial_results:\n",
    "        relevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "    \n",
    "    # 관련성 점수에 따라 결과 정렬\n",
    "    ranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # 상위 k개 결과 반환\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analytical Strategy - 폭넓은 정보 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    종합적인 정보 획득을 위한 분석 쿼리에 대한 검색 전략\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 반환할 문서의 수\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서\n",
    "    \"\"\"\n",
    "    print(f\"Executing Analytical retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # 시스템 프롬프트 \n",
    "    system_prompt = \"\"\"You are an expert at breaking down complex questions.\n",
    "    Generate sub-questions that explore different aspects of the main analytical query.\n",
    "    These sub-questions should cover the breadth of the topic and help retrieve \n",
    "    comprehensive information.\n",
    "\n",
    "    Return a list of exactly 3 sub-questions, one per line.\n",
    "    \"\"\"\n",
    "\n",
    "    # 사용자 프롬프트\n",
    "    user_prompt = f\"Generate sub-questions for this analytical query: {query}\"\n",
    "    \n",
    "    # 서브 쿼리 생성\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # 생성된 서브 쿼리 추출\n",
    "    sub_queries = response.choices[0].message.content.strip().split('\\n')\n",
    "    sub_queries = [q.strip() for q in sub_queries if q.strip()]\n",
    "    print(f\"Generated sub-queries: {sub_queries}\")\n",
    "    \n",
    "    # 서브 쿼리로 문서 검색\n",
    "    all_results = []\n",
    "    for sub_query in sub_queries:\n",
    "        sub_query_embedding = create_embeddings(embedding_model, [sub_query], device=device, batch_size=1)[0]\n",
    "        # 유사 문서 검색\n",
    "        results = vector_store.similarity_search(sub_query_embedding, k=2)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # 서브 쿼리 결과에서 다양성 보장\n",
    "    # 중복 제거 (동일한 텍스트 내용)\n",
    "    unique_texts = set()\n",
    "    diverse_results = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result[\"text\"] not in unique_texts:\n",
    "            unique_texts.add(result[\"text\"])\n",
    "            diverse_results.append(result)\n",
    "    \n",
    "    # k개의 결과를 얻기 위해 초기 결과에서 더 많은 결과 추가\n",
    "    if len(diverse_results) < k:\n",
    "        # 메인 쿼리에 대한 직접 검색\n",
    "        main_query_embedding = create_embeddings(embedding_model, [query], device=device, batch_size=1)[0]\n",
    "        main_results = vector_store.similarity_search(main_query_embedding, k=k)\n",
    "        \n",
    "        for result in main_results:\n",
    "            if result[\"text\"] not in unique_texts and len(diverse_results) < k:\n",
    "                unique_texts.add(result[\"text\"])\n",
    "                diverse_results.append(result)\n",
    "    \n",
    "    # 상위 k개 다양한 결과 반환\n",
    "    return diverse_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Opinion Strategy - 댜앙한 관점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    다양한 관점을 반영하기 위해, 의견형 질문에는 균형 잡힌 시각을 담은 자료들을 폭넓게 수집하는 검색 전략을 사용합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 반환할 문서의 수\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서\n",
    "    \"\"\"\n",
    "    print(f\"Executing Opinion retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"You are an expert at identifying different perspectives on a topic.\n",
    "        For the given query about opinions or viewpoints, identify different perspectives \n",
    "        that people might have on this topic.\n",
    "\n",
    "        Return a list of exactly 3 different viewpoint angles, one per line.\n",
    "    \"\"\"\n",
    "\n",
    "    # 사용자 프롬프트 생성\n",
    "    user_prompt = f\"Identify different perspectives on: {query}\"\n",
    "    \n",
    "    # 다양한 관점 생성\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # 관점 추출\n",
    "    viewpoints = response.choices[0].message.content.strip().split('\\n')\n",
    "    viewpoints = [v.strip() for v in viewpoints if v.strip()]\n",
    "    print(f\"Identified viewpoints: {viewpoints}\")\n",
    "    \n",
    "    # 각 viewpoint을 나타내는 문서 검색\n",
    "    all_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "        # 메인 쿼리와 viewpoint을 결합\n",
    "        combined_query = f\"{query} {viewpoint}\"\n",
    "        # 결합된 쿼리의 임베딩 생성\n",
    "        viewpoint_embedding = create_embeddings(embedding_model, [combined_query], device=device, batch_size=1)[0]\n",
    "        # 결합된 쿼리에 대한 유사도 검색\n",
    "        results = vector_store.similarity_search(viewpoint_embedding, k=2)\n",
    "        \n",
    "        # 결과에 관점을 표시\n",
    "        for result in results:\n",
    "            result[\"viewpoint\"] = viewpoint\n",
    "        \n",
    "        # 결과 추가\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # 다양한 관점 선택\n",
    "    selected_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "        viewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint]\n",
    "        if viewpoint_docs:\n",
    "            selected_results.append(viewpoint_docs[0])\n",
    "    \n",
    "    # 아직 채워지지 않은 자리를 유사도가 가장 높은 문서들로 채움.\n",
    "    remaining_slots = k - len(selected_results)\n",
    "    if remaining_slots > 0:\n",
    "        # 남은 문서들을 유사도 기준으로 정렬\n",
    "        remaining_docs = [r for r in all_results if r not in selected_results]\n",
    "        remaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        selected_results.extend(remaining_docs[:remaining_slots])\n",
    "    \n",
    "    # 상위 k개 결과 반환\n",
    "    return selected_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Contextual Strategy - User Context 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_retrieval_strategy(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    사용자 맥락을 통합한 맥락 기반 쿼리에 대한 검색 전략\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 반환할 문서의 수\n",
    "        user_context (str): 추가 사용자 맥락\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 검색된 문서\n",
    "    \"\"\"\n",
    "    print(f\"Executing Contextual retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # 사용자 맥락이 제공되지 않으면 쿼리에서 유추\n",
    "    if not user_context:\n",
    "        system_prompt = \"\"\"You are an expert at understanding implied context in questions.\n",
    "For the given query, infer what contextual information might be relevant or implied \n",
    "but not explicitly stated. Focus on what background would help answering this query.\n",
    "\n",
    "Return a brief description of the implied context.\"\"\"\n",
    "\n",
    "        user_prompt = f\"Infer the implied context in this query: {query}\"\n",
    "        \n",
    "        # Generate the inferred context using the LLM\n",
    "        response = client_openai.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # inferred context 추출 및 출력\n",
    "        user_context = response.choices[0].message.content.strip()\n",
    "        print(f\"Inferred context: {user_context}\")\n",
    "    \n",
    "    # 사용자 context를 포함하도록 쿼리 재구성\n",
    "    system_prompt = \"\"\"You are an expert at reformulating questions with context.\n",
    "    Given a query and some contextual information, create a more specific query that \n",
    "    incorporates the context to get more relevant information.\n",
    "\n",
    "    Return ONLY the reformulated query without explanation.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {user_context}\n",
    "\n",
    "    Reformulate the query to incorporate this context:\"\"\"\n",
    "    \n",
    "    # LLM을 사용하여 맥락화된 쿼리 생성\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 맥락화된 쿼리 추출 및 출력\n",
    "    contextualized_query = response.choices[0].message.content.strip()\n",
    "    print(f\"Contextualized query: {contextualized_query}\")\n",
    "    \n",
    "    # 유사 문서 검색\n",
    "    query_embedding = create_embeddings(embedding_model, [contextualized_query], device=device, batch_size=1)[0]\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    # 관련성과 사용자 맥락을 고려하여 문서 순위 매기기\n",
    "    ranked_results = []\n",
    "    \n",
    "    for doc in initial_results:\n",
    "        # 맥락을 고려하여 문서 관련성 점수 계산 \n",
    "        context_relevance = score_document_context_relevance(query, user_context, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"context_relevance\": context_relevance\n",
    "        })\n",
    "    \n",
    "    # 맥락 관련성에 따라 결과 정렬\n",
    "    ranked_results.sort(key=lambda x: x[\"context_relevance\"], reverse=True)\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Document Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_document_relevance(query, document, model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    LLM을 사용하여 문서 관련성 점수 계산.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        document (str): Document text\n",
    "        model (str): LLM model\n",
    "        \n",
    "    Returns:\n",
    "        float: 0-10 범위의 관련성 점수\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"You are an expert at evaluating document relevance.\n",
    "        Rate the relevance of a document to a query on a scale from 0 to 10, where:\n",
    "        0 = Completely irrelevant\n",
    "        10 = Perfectly addresses the query\n",
    "\n",
    "        Return ONLY a numerical score between 0 and 10, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    # 문서가 너무 길면 truncate\n",
    "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
    "    \n",
    "    # 쿼리와 문서 미리보기가 포함된 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "\n",
    "        Document: {doc_preview}\n",
    "\n",
    "        Relevance score (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # 응답 생성\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 응답에서 점수 추출\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 점수 추출\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "        return min(10, max(0, score))  # 0-10 범위로 제한\n",
    "    else:\n",
    "        # 추출 실패 시 기본 점수\n",
    "        return 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_document_context_relevance(query, context, document, model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    쿼리와 맥락을 고려하여 문서 관련성 점수 계산\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): User context\n",
    "        document (str): Document text\n",
    "        model (str): LLM model\n",
    "        \n",
    "    Returns:\n",
    "        float: 0-10 범위의 관련성 점수\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"You are an expert at evaluating document relevance considering context.\n",
    "        Rate the document on a scale from 0 to 10 based on how well it addresses the query\n",
    "        when considering the provided context, where:\n",
    "        0 = Completely irrelevant\n",
    "        10 = Perfectly addresses the query in the given context\n",
    "\n",
    "        Return ONLY a numerical score between 0 and 10, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    # 문서가 너무 길면 truncate\n",
    "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
    "    \n",
    "    # 쿼리, 맥락, 문서 미리보기가 포함된 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {context}\n",
    "\n",
    "    Document: {doc_preview}\n",
    "\n",
    "    Relevance score considering context (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # 응답 생성 \n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 응답에서 점수 추출 \n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 점수 추출\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "        return min(10, max(0, score))  # 0-10 범위로 제한\n",
    "    else:\n",
    "        # 추출 실패 시 기본 점수\n",
    "        return 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Adaptive Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_retrieval(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    적절한 전략을 선택하여 적응형 검색 수행\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리    \n",
    "        vector_store (SimpleVectorStore): 벡터 저장소\n",
    "        k (int): 검색할 문서 수\n",
    "        user_context (str): 상황에 따라, 문맥형 질문에 사용자 정보를 선택적으로 반영할 수 있음\n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    # 쿼리 유형 분류\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "    \n",
    "    # 쿼리 유형에 따라 적절한 검색 전략 선택 및 실행\n",
    "    if query_type == \"Factual\":\n",
    "        # 정확한 정보를 위한 사실 기반 검색 전략\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Analytical\":\n",
    "        # 종합적인 정보 획득을 위한 분석 쿼리에 대한 검색 전략\n",
    "        results = analytical_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Opinion\":\n",
    "        # 다양한 관점을 위한 의견 기반 검색 전략\n",
    "        results = opinion_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Contextual\":\n",
    "        # 사용자 맥락을 포함하는 맥락 기반 검색 전략\n",
    "        results = contextual_retrieval_strategy(query, vector_store, k, user_context)\n",
    "    else:\n",
    "        # 분류 실패 시 기본 사실 기반 검색 전략\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    \n",
    "    return results  # 검색된 문서 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, results, query_type, model_name=\"gpt-4.1-nano\"):\n",
    "    \"\"\"\n",
    "    질문 내용, 검색된 문서들, 그리고 질문의 유형을 바탕으로 가장 적절한 답변을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Retrieved documents\n",
    "        query_type (str): Type of query\n",
    "        model_name (str): LLM model\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # 검색된 문서의 텍스트를 구분자로 결합하여 맥락 준비\n",
    "    context = \"\\n\\n---\\n\\n\".join([r[\"text\"] for r in results])\n",
    "    \n",
    "    # 쿼리 유형에 따라 시스템 프롬프트 생성\n",
    "    if query_type == \"Factual\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing factual information.\n",
    "    Answer the question based on the provided context. Focus on accuracy and precision.\n",
    "    If the context doesn't contain the information needed, acknowledge the limitations.\"\"\"\n",
    "        \n",
    "    elif query_type == \"Analytical\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing analytical insights.\n",
    "    Based on the provided context, offer a comprehensive analysis of the topic.\n",
    "    Cover different aspects and perspectives in your explanation.\n",
    "    If the context has gaps, acknowledge them while providing the best analysis possible.\"\"\"\n",
    "        \n",
    "    elif query_type == \"Opinion\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant discussing topics with multiple viewpoints.\n",
    "    Based on the provided context, present different perspectives on the topic.\n",
    "    Ensure fair representation of diverse opinions without showing bias.\n",
    "    Acknowledge where the context presents limited viewpoints.\"\"\"\n",
    "        \n",
    "    elif query_type == \"Contextual\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing contextually relevant information.\n",
    "    Answer the question considering both the query and its context.\n",
    "    Make connections between the query context and the information in the provided documents.\n",
    "    If the context doesn't fully address the specific situation, acknowledge the limitations.\"\"\"\n",
    "        \n",
    "    else:\n",
    "        system_prompt = \"\"\"You are a helpful assistant. Answer the question based on the provided context. If you cannot answer from the context, acknowledge the limitations.\"\"\"\n",
    "    \n",
    "    # 컨텍스트와 쿼리를 결합하여 사용자 프롬프트 생성\n",
    "    user_prompt = f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Please provide a helpful response based on the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 응답 생성\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답 내용 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with Adaptive Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_adaptive_retrieval(file_path, query, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Adaptive RAG 파이프라인\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 파일 경로\n",
    "        query (str): 사용자 쿼리\n",
    "        k (int): 반환할 문서의 수\n",
    "        user_context (str): 선택적 사용자 맥락\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including query, retrieved documents, query type, and response\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RAG WITH ADAPTIVE RETRIEVAL ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # 문서를 처리하여 텍스트 추출, 청크로 나누고 임베딩 생성\n",
    "    chunks, vector_store = process_document(file_path)\n",
    "    \n",
    "    # 쿼리 유형 분류\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "    \n",
    "    # 쿼리 유형에 따라 적절한 검색 전략 선택 및 실행\n",
    "    retrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n",
    "    \n",
    "    # 쿼리, 검색된 문서, 쿼리 유형에 따라 응답 생성\n",
    "    response = generate_response(query, retrieved_docs, query_type)\n",
    "    \n",
    "    # 질문에 대한 전체 결과 저장\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"query_type\": query_type,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adaptive_vs_standard(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    테스트 쿼리 세트에 대해 adaptive RAG와 Standard RAG를 비교합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): 파일 경로\n",
    "        test_queries (List[str]): 평가할 쿼리 목록\n",
    "        reference_answers (List[str], optional): 평가 메트릭을 위한 참조 답변\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 개별 쿼리 결과와 전체 비교 결과가 포함된 평가 결과\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL ===\")\n",
    "    \n",
    "    # 문서를 처리하여 텍스트 추출, 청크로 나누고 벡터 저장소 생성\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # 비교 결과를 저장할 컬렉션 초기화\n",
    "    results = []\n",
    "    \n",
    "    # 각 테스트 쿼리에 대해 Standard 검색과 adaptive 검색 모두 처리\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\nQuery {i+1}: {query}\")\n",
    "        \n",
    "        # --- Standard retrieval approach ---\n",
    "        print(\"\\n--- Standard Retrieval ---\")\n",
    "        # 쿼리의 임베딩 생성\n",
    "        query_embedding = create_embeddings(embedding_model, [query], device=device, batch_size=1)[0]\n",
    "        # 단순 벡터 유사도를 사용하여 문서 검색\n",
    "        standard_docs = vector_store.similarity_search(query_embedding, k=4)\n",
    "        # 일반적인 접근 방식을 사용하여 응답 생성\n",
    "        standard_response = generate_response(query, standard_docs, \"General\")\n",
    "        \n",
    "        # --- Adaptive retrieval approach ---\n",
    "        print(\"\\n--- Adaptive Retrieval ---\")\n",
    "        # 쿼리 유형 분류 (Factual, Analytical, Opinion, Contextual)\n",
    "        query_type = classify_query(query)\n",
    "        # 쿼리 유형에 따라 적절한 검색 전략 선택 및 실행\n",
    "        adaptive_docs = adaptive_retrieval(query, vector_store, k=4)\n",
    "        # 쿼리 유형에 맞는 응답 생성\n",
    "        adaptive_response = generate_response(query, adaptive_docs, query_type)\n",
    "        \n",
    "        # 쿼리에 대한 완전한 결과 저장\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"query_type\": query_type,\n",
    "            \"standard_retrieval\": {\n",
    "                \"documents\": standard_docs,\n",
    "                \"response\": standard_response\n",
    "            },\n",
    "            \"adaptive_retrieval\": {\n",
    "                \"documents\": adaptive_docs,\n",
    "                \"response\": adaptive_response\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 참조 답변이 있는 경우 추가\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            result[\"reference_answer\"] = reference_answers[i]\n",
    "            \n",
    "        results.append(result)\n",
    "        \n",
    "        # 빠른 비교를 위한 두 응답의 미리보기 표시\n",
    "        print(\"\\n--- Responses ---\")\n",
    "        print(f\"Standard: {standard_response[:200]}...\")\n",
    "        print(f\"Adaptive: {adaptive_response[:200]}...\")\n",
    "    \n",
    "    # 기준 답안이 있을 경우, 비교 분석을 위한 지표를 산출합니다.\n",
    "    if reference_answers:\n",
    "        comparison = compare_responses(results)\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(comparison)\n",
    "    \n",
    "    # 완전한 평가 결과 반환\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"comparison\": comparison if reference_answers else \"No reference answers provided for evaluation\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(results):\n",
    "    \"\"\"\n",
    "    표준 검색과 적응형 검색 응답을 참조 답변과 비교.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results containing both types of responses\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트\n",
    "    comparison_prompt = \"\"\"You are an expert evaluator of information retrieval systems.\n",
    "    Compare the standard retrieval and adaptive retrieval responses for each query.\n",
    "    Consider factors like accuracy, relevance, comprehensiveness, and alignment with the reference answer.\n",
    "    Provide a detailed analysis of the strengths and weaknesses of each approach.\"\"\"\n",
    "    \n",
    "    # 비교 텍스트 초기화\n",
    "    comparison_text = \"# Evaluation of Standard vs. Adaptive Retrieval\\n\\n\"\n",
    "    \n",
    "    # 각 결과를 비교\n",
    "    for i, result in enumerate(results):\n",
    "        # 참조 답변이 없는 경우 건너뛰기\n",
    "        if \"reference_answer\" not in result:\n",
    "            continue\n",
    "            \n",
    "        # 쿼리 세부 정보를 비교 텍스트에 추가   \n",
    "        comparison_text += f\"## Query {i+1}: {result['query']}\\n\"\n",
    "        comparison_text += f\"*Query Type: {result['query_type']}*\\n\\n\"\n",
    "        comparison_text += f\"**Reference Answer:**\\n{result['reference_answer']}\\n\\n\"\n",
    "        \n",
    "        # 표준 검색 응답을 비교 텍스트에 추가\n",
    "        comparison_text += f\"**Standard Retrieval Response:**\\n{result['standard_retrieval']['response']}\\n\\n\"\n",
    "        \n",
    "        # 적응형 검색 응답을 비교 텍스트에 추가\n",
    "        comparison_text += f\"**Adaptive Retrieval Response:**\\n{result['adaptive_retrieval']['response']}\\n\\n\"\n",
    "        \n",
    "        # 응답을 비교하기 위한 사용자 프롬프트 생성\n",
    "        user_prompt = f\"\"\"\n",
    "        Reference Answer: {result['reference_answer']}\n",
    "        \n",
    "        Standard Retrieval Response: {result['standard_retrieval']['response']}\n",
    "        \n",
    "        Adaptive Retrieval Response: {result['adaptive_retrieval']['response']}\n",
    "        \n",
    "        Provide a detailed comparison of the two responses.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 비교 분석 생성\n",
    "        response = client_openai.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": comparison_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        # AI가 생성한 비교 분석 내용을 기존의 비교 텍스트에 추가합니다.\n",
    "        comparison_text += f\"**Comparison Analysis:**\\n{response.choices[0].message.content}\\n\\n\"\n",
    "    \n",
    "    return comparison_text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Adaptive Retrieval System (Customized Queries)\n",
    "\n",
    "The final step to use the adaptive RAG evaluation system is to call the evaluate_adaptive_vs_standard() function with your PDF document and test queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 평가 데이터 로드\n",
    "df = pd.read_csv('./data_creation/rag_val_new_post.csv')\n",
    "\n",
    "# 테스트 쿼리 정의\n",
    "test_queries = [\n",
    "    \"연금소득의 분리과세 기준금액은 얼마로 인상되었습니까?\",# Factual query - seeking definition/specific information\n",
    "    # \"조정대상조세 금액의 합계액이 음수일 때, 추가세액비율이 최저한세율을 초과하는 문제를 어떻게 해소할 수 있습니까?\" # analsis query - requiring comprehensive analysis\n",
    "    # \"가상자산 과세 유예 조치는 납세자 보호 측면에서 정당하다고 볼 수 있을까요?\", # Opinion query - seeking diverse perspectives\n",
    "    # \"여자 코일은 선행발명의 코일과 동일한가요?\", # Contextual query - benefits from context-awareness\n",
    "]\n",
    "\n",
    "# 참조 답변 정의\n",
    "reference_answers = [\n",
    "    \"연금소득의 분리과세 기준금액은 1천200만원에서 1천500만원으로 인상되었습니다.\",\n",
    "    # \"조정대상조세 금액의 합계액이 음수일 때, 추가세액비율이 최저한세율을 초과하는 문제를 해소하기 위해 해당 사업연도 실효세율을 영(零)으로 보고, 이 금액은 해당 사업연도 실효세율 계산에 산입됩니다.\",\n",
    "    # \"가상자산 과세 유예 조치가 납세자 보호라는 관점에서 정당한가에 대해서는 시각에 따라 평가가 다를 수 있습니다.\",\n",
    "    # \"네, 여자 코일은 선행발명의 코일과 동일합니다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive 검색과 표준 검색 비교 평가\n",
    "# 각 쿼리를 두 가지 방법으로 처리하고 결과를 비교합니다.\n",
    "# 파일 경로\n",
    "file_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "evaluation_results = evaluate_adaptive_vs_standard(\n",
    "    pdf_path=file_path,                  # 지식 추출을 위한 소스 문서\n",
    "    test_queries=test_queries,          # 평가할 테스트 쿼리 목록\n",
    "    reference_answers=reference_answers  # 비교를 위한 참조 답변\n",
    ")\n",
    "\n",
    "# 성능 비교\n",
    "print(evaluation_results[\"comparison\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
