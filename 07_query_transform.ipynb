{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Query Transformations for Enhanced RAG Systems\n",
    "\n",
    "이 노트북에서는 LangChain 같은 특수 라이브러리 없이도 RAG 시스템의 검색 성능을 높일 수 있는 세 가지 쿼리 변환 기법을 소개합니다.   \n",
    "사용자의 질문을 조금만 바꿔도, 더 관련성 높고 풍부한 정보를 쉽게 찾을 수 있습니다.\n",
    " \n",
    "#### 주요 쿼리 변환 방법\n",
    "\n",
    "1. **Query Rewriting**: 질문을 더 구체적이고 자세하게 바꿔서 검색 정확도를 높입니다.\n",
    "2. **Step-back Prompting**: 질문을 더 넓은 범위로 확장해, 배경이나 맥락 정보를 함께 찾을 수 있게 합니다.\n",
    "3. **Sub-query Decomposition**: 복잡한 질문을 여러 개의 간단한 질문으로 나눠서, 더 폭넓고 꼼꼼하게 정보를 검색합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정하기\n",
    "필요한 라이브러리를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API 클라이언트 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client_openai = OpenAI(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Transformation 구현하기\n",
    "### 1. Query Rewriting\n",
    "\n",
    "`Query Rewriting`은 사용자의 질문을 구체적이고 자세하게 바꿔서, 원하는 정보를 확하게 찾을 수 있게 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query, model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    사용자의 질문을 구체적이고 자세하게 변환.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): 원본 사용자 쿼리\n",
    "        model_name (str): 쿼리 리라이팅에 사용할 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 리라이팅된 쿼리\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"You are an AI assistant specialized in improving search queries. Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information in korea.\"\n",
    "    \n",
    "    # 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"\n",
    "    Rewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Rewritten query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 지정된 모델을 사용하여 Query Rewriting\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0.0,  # temperature는 매우 낮게 설정\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 리라이팅된 쿼리를 반환하고, 앞뒤 공백을 제거합니다.\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Step-back Prompting\n",
    "더 넓은 범위의 쿼리를 만들어서 배경 정보나 맥락을 파악하는 데 도움을 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step_back_query(original_query, model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    더 넓은 범위의 쿼리 생성\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): 원본 사용자 쿼리\n",
    "        model_name (str): 스텝백 쿼리 생성에 사용할 모델\n",
    "        \n",
    "    Returns:\n",
    "        str: 스텝백 쿼리\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"You are an AI assistant specialized in search strategies. Your task is to generate broader in korea, more general versions of specific queries to retrieve relevant background information.\"\n",
    "    \n",
    "    # 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"\n",
    "    Generate a broader, more general version of the following query that could help retrieve useful background information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Step-back query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 지정된 모델을 사용하여 step-back query 생성\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0.1,  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 스텝백 쿼리를 return하고, 앞뒤 공백 제거\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sub-query Decomposition\n",
    "복잡한 쿼리를 더 단순한 질문들로 나누어, 다양한 측면에서 정보를 폭넓게 찾을 수 있도록 도와줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(original_query, num_subqueries=4, model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    복잡한 쿼리를 더 간단한 서브쿼리로 분해합니다.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): 원본 복잡한 쿼리\n",
    "        num_subqueries (int): 생성할 서브쿼리의 수\n",
    "        model_name (str): 쿼리 분해에 사용할 모델\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: 더 간단한 서브쿼리 리스트\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"You are an AI assistant specialized in breaking down complex questions. Your task is to decompose complex queries into simpler sub-questions that, when answered together, address the original query in korea\"\n",
    "    \n",
    "    # 사용자 프롬프트\n",
    "    user_prompt = f\"\"\"\n",
    "    Break down the following complex query into {num_subqueries} simpler sub-queries. Each sub-query should focus on a different aspect of the original question.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Generate {num_subqueries} sub-queries, one per line, in this format:\n",
    "    1. [First sub-query]\n",
    "    2. [Second sub-query]\n",
    "    And so on...\n",
    "    \"\"\"\n",
    "    \n",
    "    # 지정된 모델을 사용하여 서브쿼리를 생성합니다.\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0.2,  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 응답에서 서브쿼리 추출\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 번호가 붙은 쿼리 추출\n",
    "    lines = content.split(\"\\n\")\n",
    "    sub_queries = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n",
    "            query = line.strip()\n",
    "            query = query[query.find(\".\")+1:].strip()\n",
    "            sub_queries.append(query)\n",
    "    \n",
    "    return sub_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 쿼리 변환 기법 비교\n",
    "다양한 쿼리 변환 기법을 예시 쿼리에 적용해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 쿼리\n",
    "original_query = \"AI가 미래에 어떤 직업들에게 영향을 미칠까요?\"\n",
    "\n",
    "# 쿼리 변환 적용\n",
    "print(\"Original Query:\", original_query)\n",
    "\n",
    "# Query Rewriting\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "print(\"\\n1. Rewritten Query:\")\n",
    "print(rewritten_query)\n",
    "\n",
    "# Step-back Prompting\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "print(\"\\n2. Step-back Query:\")\n",
    "print(step_back_query)\n",
    "\n",
    "# Sub-query Decomposition\n",
    "sub_queries = decompose_query(original_query, num_subqueries=4)\n",
    "print(\"\\n3. Sub-queries:\")\n",
    "for i, query in enumerate(sub_queries, 1):\n",
    "    print(f\"   {i}. {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store 구축\n",
    "NumPy를 사용하여 간단한 Vecotr store 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용하여 간단한 Vecotr store 구축\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소 초기화\n",
    "        \"\"\"\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목 추가\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        시맨틱 서치 수행\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): 쿼리 임베딩 벡터.\n",
    "        k (int): 반환할 결과의 수.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: 텍스트와 메타데이터가 포함된 상위 k개 유사 항목.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # 쿼리 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 유사도에 따라 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def create_embeddings(embedding_model, texts, device='cuda', batch_size=16):\n",
    "    \"\"\"\n",
    "    SentenceTransformer 모델을 사용하여 지정된 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        embedding_model: 임베딩을 생성할 SentenceTransformer 모델입니다.\n",
    "        texts (list): 임베딩을 생성할 입력 텍스트 리스트입니다.\n",
    "        device (str): 모델을 실행할 장치 ('cuda' for GPU, 'cpu' for CPU).\n",
    "        batch_size (int): 인코딩을 위한 배치 크기입니다.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 모델에 의해 생성된 임베딩입니다.\n",
    "    \"\"\"\n",
    "    # 모델이 지정된 장치에 있는지 확인합니다.\n",
    "    embedding_model = embedding_model.to(device)\n",
    "    \n",
    "    # 지정된 배치 크기로 임베딩을 생성합니다.\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        device=device,\n",
    "        batch_size=batch_size,  # 메모리 사용량을 줄이기 위해 더 작은 배치 크기를 사용합니다.\n",
    "        show_progress_bar=True  # 인코딩 진행 상태를 모니터링하기 위한 진행 표시줄을 표시합니다.\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# GPU 사용 가능 여부를 확인합니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델을 로드합니다.\n",
    "model = \"BAAI/bge-m3\"\n",
    "embedding_model = SentenceTransformer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 쿼리 변환을 적용한 RAG 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # API 키 설정\n",
    "    genai.configure(api_key=gemini_API_KEY)\n",
    "    client = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "\n",
    "    # PDF 파일 업로드\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        file_data = file.read()\n",
    "\n",
    "\n",
    "    prompt = \"Extract all text from the provided PDF file.\"\n",
    "    response = client.generate_content([\n",
    "        {\"mime_type\": \"application/pdf\", \"data\": file_data},\n",
    "        prompt\n",
    "    ],generation_config={\n",
    "            \"max_output_tokens\": 8192  # 최대 출력 토큰 수 설정 (예: 8192 토큰, 약 24,000~32,000자)\n",
    "    })\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 text 파일로 저장되어 있다면 load_text_file 함수를 사용하면 됩니다.\n",
    "def load_text_file(pdf_path):\n",
    "\n",
    "    # text 파일 로드\n",
    "    with open(pdf_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "        text = txt_file.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "txt_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "extracted_text = load_text_file(txt_path)\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n자 단위로, 일부가 겹치도록 chunking 합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청크할 텍스트입니다.\n",
    "    n (int): 각 청크의 문자 수입니다.\n",
    "    overlap (int): 청크 간 겹치는 문자 수입니다.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 청크된 텍스트 리스트입니다.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크된 텍스트를 저장할 리스트\n",
    "    \n",
    "    # overlap만큼 겹치도록 text를 n의 길이로 chunking\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    질문 생성을 통한 Document Augmentation \n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF 파일 경로.\n",
    "    chunk_size (int): 청크의 길이.\n",
    "    chunk_overlap (int): 청크 간 겹치는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], SimpleVectorStore]: 텍스트 청크와 벡터 저장소.\n",
    "    \"\"\"\n",
    "    # print(\"Extracting text from PDF...\")\n",
    "    # extracted_text = extract_text_from_pdf(file_path)\n",
    "\n",
    "    # 텍스트 파일 로드\n",
    "    extracted_text = load_text_file(file_path)\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # 텍스트 청크의 임베딩 생성\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(embedding_model, chunks, device=device, batch_size=1)\n",
    "    \n",
    "    # 벡터 저장소 생성\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # 각 청크와 임베딩을 VectorStore에 저장\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": file_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_search(query, vector_store, transformation_type, top_k=3):\n",
    "    \"\"\"\n",
    "    변환된 쿼리를 사용하여 유사 텍스트(청크) 검색\n",
    "    \n",
    "    Args:\n",
    "        query (str): 원본 쿼리\n",
    "        vector_store (SimpleVectorStore): 검색할 벡터 저장소\n",
    "        transformation_type (str): 변환 유형 ('rewrite', 'step_back', or 'decompose')\n",
    "        top_k (int): 반환할 결과의 수\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 검색 결과\n",
    "    \"\"\"\n",
    "    print(f\"Transformation type: {transformation_type}\")\n",
    "    print(f\"Original query: {query}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if transformation_type == \"rewrite\":\n",
    "        # Query rewriting\n",
    "        transformed_query = rewrite_query(query)\n",
    "        print(f\"Rewritten query: {transformed_query}\")\n",
    "        \n",
    "        # 변환된 쿼리의 임베딩 생성\n",
    "        query_embedding = create_embeddings(embedding_model, [transformed_query], device=device, batch_size=1)[0]\n",
    "        \n",
    "        # rewriting된 쿼리로 검색\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"step_back\":\n",
    "        # 스텝백 프롬프트\n",
    "        transformed_query = generate_step_back_query(query)\n",
    "        print(f\"Step-back query: {transformed_query}\")\n",
    "        \n",
    "        # 변환된 쿼리의 임베딩 생성\n",
    "        query_embedding = create_embeddings(embedding_model, [transformed_query], device=device, batch_size=1)[0]\n",
    "        \n",
    "        # 스텝백 쿼리로 검색\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"decompose\":\n",
    "        # 서브쿼리 분해\n",
    "        sub_queries = decompose_query(query)\n",
    "        print(\"Decomposed into sub-queries:\")\n",
    "        for i, sub_q in enumerate(sub_queries, 1):\n",
    "            print(f\"{i}. {sub_q}\")\n",
    "        \n",
    "        # 모든 서브쿼리의 임베딩 생성\n",
    "        sub_query_embeddings = create_embeddings(embedding_model, sub_queries, device=device, batch_size=1)\n",
    "        \n",
    "        # 각 서브쿼리로 검색하고 결과 결합\n",
    "        all_results = []\n",
    "        for i, embedding in enumerate(sub_query_embeddings):\n",
    "            sub_results = vector_store.similarity_search(embedding, k=2)  # 각 서브쿼리당 결과 수 줄임\n",
    "            all_results.extend(sub_results)\n",
    "        \n",
    "        # 중복 제거 (가장 높은 유사도 점수 유지)\n",
    "        seen_texts = {}\n",
    "        for result in all_results:\n",
    "            text = result[\"text\"]\n",
    "            if text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\n",
    "                seen_texts[text] = result\n",
    "        \n",
    "        # 유사도 순으로 정렬하고 top_k 결과 선택\n",
    "        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n",
    "        \n",
    "    else:\n",
    "        # 변환 없이 일반 검색\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색된 청크를 기반으로 response 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model_name='gpt-4.1-nano'):\n",
    "\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"당신은 제공된 Context에 기반하여 답변하는 AI 어시스턴트입니다. 답변이 컨텍스트에서 직접 도출될 수 없는 경우, 다음 문장을 사용하세요: '해당 질문에 답변할 충분한 정보가 없습니다.'\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please answer the question based only on the context provided above. Be concise and accurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Complete RAG Pipeline with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_query_transformation(file_path, query, transformation_type=None):\n",
    "    \"\"\"\n",
    "    쿼리 변환을 선택적으로 적용하여 전체 RAG 파이프라인 실행\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 파일 경로\n",
    "        query (str): 사용자 쿼리\n",
    "        transformation_type (str): 변환 유형 (None, 'rewrite', 'step_back', or 'decompose')\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 쿼리, 변환된 쿼리, 컨텍스트, 응답이 포함된 결과\n",
    "    \"\"\"\n",
    "    # 벡터 저장소 생성\n",
    "    vector_store = process_document(file_path)\n",
    "    \n",
    "    # 쿼리 변환 적용 및 검색\n",
    "    if transformation_type:\n",
    "        # 변환된 쿼리로 검색\n",
    "        results = transformed_search(query, vector_store, transformation_type)\n",
    "    else:\n",
    "        # 변환 없이 원본 쿼리로 검색\n",
    "        query_embedding = create_embeddings(embedding_model, [query], device=device, batch_size=1)[0]\n",
    "        results = vector_store.similarity_search(query_embedding, k=3)\n",
    "    \n",
    "    # 검색 결과에서 컨텍스트 결합\n",
    "    context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(results)])\n",
    "    \n",
    "    # 쿼리와 결합된 컨텍스트를 기반으로 응답 생성\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # 원본 쿼리, 변환 유형, 컨텍스트, 응답이 포함된 결과 반환\n",
    "    return {\n",
    "        \"original_query\": query,\n",
    "        \"transformation_type\": transformation_type,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Transformation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(results, reference_answer, model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    다른 쿼리 변환 기법의 응답을 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): 다른 변환 기법의 결과\n",
    "        reference_answer (str): 비교를 위한 기준 답변\n",
    "        model (str): 평가에 사용할 모델\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. \n",
    "    Your task is to compare different responses generated using various query transformation techniques \n",
    "    and determine which technique produced the best response compared to the reference answer.\"\"\"\n",
    "    \n",
    "    # 기준 답변과 각 기법의 응답을 비교하기 위한 텍스트 준비\n",
    "    comparison_text = f\"\"\"Reference Answer: {reference_answer}\\n\\n\"\"\"\n",
    "    \n",
    "    for technique, result in results.items():\n",
    "        comparison_text += f\"{technique.capitalize()} Query Response:\\n{result['response']}\\n\\n\"\n",
    "    \n",
    "    # 유저 프롬프트\n",
    "    user_prompt = f\"\"\"\n",
    "    {comparison_text}\n",
    "    \n",
    "    Compare the responses generated by different query transformation techniques to the reference answer.\n",
    "    \n",
    "    For each technique (original, rewrite, step_back, decompose):\n",
    "    1. Score the response from 1-10 based on accuracy, completeness, and relevance\n",
    "    2. Identify strengths and weaknesses\n",
    "    \n",
    "    Then rank the techniques from best to worst and explain which technique performed best overall and why.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 응답 평가\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 평가 결과 출력\n",
    "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"=============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformations(file_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "        다른 쿼리 변환 기법의 응답을 비교합니다.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 파일 경로\n",
    "        query (str): 평가할 쿼리\n",
    "        reference_answer (str): 비교를 위한 기준 답변\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    # 평가할 쿼리 변환 기법 정의\n",
    "    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n",
    "    results = {}\n",
    "    \n",
    "    # 각각의 방법들로 RAG pipeline실행\n",
    "    for transformation_type in transformation_types:\n",
    "        type_name = transformation_type if transformation_type else \"original\"\n",
    "        print(f\"\\n===== Running RAG with {type_name} query =====\")\n",
    "        \n",
    "        # 결과 가져오기\n",
    "        result = rag_with_query_transformation(file_path, query, transformation_type)\n",
    "        results[type_name] = result\n",
    "        \n",
    "        # 응답 출력\n",
    "        print(f\"Response with {type_name} query:\")\n",
    "        print(result[\"response\"])\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # 기준 답변이 제공된 경우 결과 비교\n",
    "    if reference_answer:\n",
    "        compare_responses(results, reference_answer)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 평가 데이터 로드하기\n",
    "df = pd.read_csv('./data_creation/rag_val_new_post.csv')\n",
    "\n",
    "# 평가 데이터에서 첫 번째 쿼리 추출\n",
    "query = df['query'][0]\n",
    "\n",
    "# 참고(정답) 답변\n",
    "reference_answer = df['generation_gt'][0]\n",
    "\n",
    "# 파일 경로\n",
    "file_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "# 응답 평가\n",
    "evaluation_results = evaluate_transformations(file_path, query, reference_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
